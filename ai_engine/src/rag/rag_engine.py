# <<<<<<< HEAD
# # import os
# # import re
# # import json
# # import logging
# # import uuid
# # from typing import List, Dict, Optional
# #
# # from config.settings import settings
# # from src.search_engine import SearchEngine
# # <<<<<<< HEAD
# # from src.rag.prompt import (
# #     SYSTEM_PROMPT,
# #     USER_PROMPT_TEMPLATE,
# #     LIBRARY_INFO,
# #     FOLLOWUP_PROMPT_TEMPLATE,
# #     SMALLTALK_PROMPT_TEMPLATE,
# #     GENERAL_QA_PROMPT_TEMPLATE
# # )
# # =======
# # from src.rag.prompt import SYSTEM_PROMPT, USER_PROMPT_TEMPLATE, LIBRARY_INFO
# # from src.rag.model_manager import ModelManager  # NEW IMPORT
# # >>>>>>> origin/main
# # from config.rag_config import (
# #     GEMINI_API_KEYS, # NEW CONFIG
# #     GEMINI_MODELS,   # NEW CONFIG
# #     GEMINI_MODEL,    # Keep for backward compatibility or default
# #     DEFAULT_TOP_K,
# #     SCORE_THRESHOLD,
# #     MIN_QUERY_LENGTH,
# #     TEMPERATURE,
# #     MAX_OUTPUT_TOKENS,
# #     QUERY_CACHE_THRESHOLD,
# #     SEARCH_EXPAND_FACTOR
# # )
# #
# #
# # # Logger cho module RAG
# # logger = logging.getLogger("RAGEngine")
# #
# #
# # class ChatSession:
# #     """
# #     L∆∞u tr·ªØ tr·∫°ng th√°i h·ªôi tho·∫°i c·ªßa m·ªôt user/session.
# #     Persists to disk to survive restarts.
# #     """
# #     def __init__(self, session_id: str):
# #         self.session_id = session_id
# #         self.history: List[Dict] = []  # [{"role": "user", "text": "..."}, ...]
# #         self.last_search_results: List[Dict] = []  # K·∫øt qu·∫£ t√¨m s√°ch g·∫ßn nh·∫•t
# #         self.file_path = os.path.join(settings.DATA_PROCESSED_DIR, "chat_sessions", f"rag_{session_id}.json")
# #
# #     def add_message(self, role: str, text: str):
# #         self.history.append({"role": role, "text": text})
# #         # FULL HISTORY: No truncation here anymore!
# #         # if len(self.history) > 20: ... (REMOVED)
# #         self.save()
# #
# #     def save(self):
# #         try:
# #             os.makedirs(os.path.dirname(self.file_path), exist_ok=True)
# #             data = {
# #                 "session_id": self.session_id,
# #                 "history": self.history,
# #                 "last_search_results": self.last_search_results
# #             }
# #             with open(self.file_path, "w", encoding="utf-8") as f:
# #                 json.dump(data, f, ensure_ascii=False, indent=2)
# #         except Exception as e:
# #             logger.error(f"Failed to save session {self.session_id}: {e}")
# #
# #     def load(self):
# #         try:
# #             if os.path.exists(self.file_path):
# #                 with open(self.file_path, "r", encoding="utf-8") as f:
# #                     data = json.load(f)
# #                     self.history = data.get("history", [])
# #                     self.last_search_results = data.get("last_search_results", [])
# #         except Exception as e:
# #             logger.error(f"Failed to load session {self.session_id}: {e}")
# #
# # class RAGEngine:
# #     """
# #     ========================================================
# #     ü§ñ RAGEngine (Improved)
# #     --------------------------------------------------------
# #     Ch·ª©c nƒÉng:
# #     - Qu·∫£n l√Ω session chat (Persistent)
# #     - Ph√¢n lo·∫°i intent (Greeting / Follow-up / Search)
# #     - RAG flow v·ªõi context memory
# #     - Multi-Model Rotation & Rate Limit Handling (NEW)
# #     ========================================================
# #     """
# #
# #     def __init__(self, top_k: int = DEFAULT_TOP_K):
# #         # 1. SEARCH ENGINE
# #         self.search_engine = SearchEngine()
# #         self.embedder = self.search_engine.embedder
# #         self.vector_db = self.search_engine.vector_db
# #         self.top_k = top_k
# #
# # <<<<<<< HEAD
# #         # Initialize Gemini client
# #         self.client = genai.Client(api_key=GEMINI_API_KEY)
# #         self.last_docs = []  # For follow-up queries
# #         self.last_books_text = ""
# #         self.history: list[dict] = []  # [{role: user/assistant, content: str}]
# #
# #     def _add_history(self, role: str, content: str, max_turns: int = 6):
# #         # L∆∞u ng·∫Øn g·ªçn l·ªãch s·ª≠ ƒë·ªÉ LLM c√≥ ng·ªØ c·∫£nh follow-up
# #         self.history.append({"role": role, "content": content})
# #         if len(self.history) > max_turns * 2:
# #             self.history = self.history[-max_turns * 2 :]
# #
# #     def _history_to_text(self) -> str:
# #         if not self.history:
# #             return "(ch∆∞a c√≥ l·ªãch s·ª≠)"
# #         lines = []
# #         for h in self.history[-8:]:
# #             prefix = "Ng∆∞·ªùi d√πng" if h["role"] == "user" else "Tr·ª£ l√Ω"
# #             lines.append(f"{prefix}: {h['content']}")
# #         return "\n".join(lines)
# #
# #     def _shorten_text(self, text: str | None, max_len: int = 480) -> str:
# #         if not text:
# #             return "(kh√¥ng c√≥ m√¥ t·∫£)"
# #         text = text.strip()
# #         return text[:max_len] + "‚Ä¶" if len(text) > max_len else text
# #
# #     def _books_to_context(self, docs: List[Dict]) -> str:
# #         lines = []
# #         for i, b in enumerate(docs, 1):
# #             snippet = self._shorten_text(b.get("richtext") or b.get("snippet"))
# #             lines.append(
# #                 f"{i}. {b.get('title')} ‚Äì {b.get('authors')} ({b.get('publish_year')})\n{snippet}"
# #             )
# #         return "\n\n".join(lines)
# #
# #     def is_smalltalk(self, question: str) -> bool:
# #         """
# #         Nh·∫≠n di·ªán c√¢u h·ªèi smalltalk / ch√†o h·ªèi.
# #         H·ªó tr·ª£ c·∫£ ti·∫øng Vi·ªát c√≥ d·∫•u v√† kh√¥ng d·∫•u.
# #         """
# #         q = question.lower().strip()
# #
# #         # Lo·∫°i b·ªè d·∫•u c√¢u
# #         q = re.sub(r'[?.!,;:]', '', q)
# #
# #         smalltalk_keywords = [
# #             # Ch√†o h·ªèi c√≥ d·∫•u
# #             "xin ch√†o", "ch√†o b·∫°n", "ch√†o", "ch√†o bu·ªïi s√°ng", "ch√†o bu·ªïi t·ªëi",
# #             # Ch√†o h·ªèi kh√¥ng d·∫•u
# #             "xin chao", "chao ban", "chao", "chao buoi sang", "chao buoi toi",
# #             # Ti·∫øng Anh
# #             "hello", "hi", "hey", "good morning", "good afternoon", "good evening",
# #             # C·∫£m ∆°n c√≥ d·∫•u
# #             "c·∫£m ∆°n", "c√°m ∆°n", "c·∫£m ∆°n b·∫°n", "c√°m ∆°n b·∫°n",
# #             # C·∫£m ∆°n kh√¥ng d·∫•u
# #             "cam on", "cam on ban",
# #             # Ti·∫øng Anh
# #             "thank", "thanks", "thank you", "tks", "ty",
# #             # T·∫°m bi·ªát c√≥ d·∫•u
# #             "t·∫°m bi·ªát", "h·∫πn g·∫∑p l·∫°i", "g·∫∑p l·∫°i sau",
# #             # T·∫°m bi·ªát kh√¥ng d·∫•u
# #             "tam biet", "hen gap lai", "gap lai sau",
# #             # Ti·∫øng Anh
# #             "bye", "goodbye", "see you", "see ya",
# #             # H·ªèi thƒÉm c√≥ d·∫•u
# #             "b·∫°n l√† ai", "t√™n g√¨", "kh·ªèe kh√¥ng", "b·∫°n ·ªïn kh√¥ng", "b·∫°n c√≥ kh·ªèe kh√¥ng",
# #             # H·ªèi thƒÉm kh√¥ng d·∫•u
# #             "ban la ai", "ten gi", "khoe khong", "ban on khong", "ban co khoe khong",
# #             # H·ªèi thƒÉm ti·∫øng Anh
# #             "how are you", "what's up", "who are you", "what is your name",
# #             # C√°c c√¢u ƒë∆°n gi·∫£n
# #             "alo", "yo", "hii", "hiii", "helloo", "helo"
# #         ]
# #
# #         # Ki·ªÉm tra exact match tr∆∞·ªõc
# #         if q in smalltalk_keywords:
# #             return True
# #
# #         # Ki·ªÉm tra contains
# #         return any(k in q for k in smalltalk_keywords)
# #
# #     def smalltalk_answer(self, question: str) -> str:
# #         """
# #         D√πng Gemini tr·∫£ l·ªùi smalltalk m·ªôt c√°ch th√¥ng minh v√† t·ª± nhi√™n.
# #         """
# #         prompt = SMALLTALK_PROMPT_TEMPLATE.format(
# #             history=self._history_to_text(),
# #             question=question
# #         )
# #
# #         try:
# #             response = self.client.models.generate_content(
# #                 model=GEMINI_MODEL,
# #                 contents=prompt,
# #                 config=types.GenerateContentConfig(
# #                     temperature=0.7,  # Cao h∆°n ƒë·ªÉ tr·∫£ l·ªùi t·ª± nhi√™n h∆°n
# #                     max_output_tokens=150  # Ng·∫Øn g·ªçn
# #                 )
# #             )
# #             return response.text.strip() if response and response.text else "üëã Ch√†o b·∫°n! M√¨nh c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?"
# #         except Exception as e:
# #             logger.error(f"Gemini smalltalk error: {e}")
# #             # Fallback n·∫øu API l·ªói
# #             return "üëã Ch√†o b·∫°n! M√¨nh l√† tr·ª£ l√Ω th∆∞ vi·ªán AI. B·∫°n c·∫ßn t√¨m s√°ch g√¨ h√¥m nay?"
# #
# #     def general_llm_answer(self, question: str) -> str:
# #         """
# #         D√πng Gemini tr·∫£ l·ªùi c√°c c√¢u h·ªèi t·ªïng qu√°t khi kh√¥ng t√¨m th·∫•y s√°ch ph√π h·ª£p.
# #         """
# #         prompt = GENERAL_QA_PROMPT_TEMPLATE.format(
# #             history=self._history_to_text(),
# #             question=question
# #         )
# #
# #         try:
# #             response = self.client.models.generate_content(
# #                 model=GEMINI_MODEL,
# #                 contents=prompt,
# #                 config=types.GenerateContentConfig(
# #                     temperature=0.5,
# #                     max_output_tokens=MAX_OUTPUT_TOKENS
# #                 )
# #             )
# #             return response.text.strip() if response and response.text else "‚ùå T√¥i ch∆∞a c√≥ c√¢u tr·∫£ l·ªùi ph√π h·ª£p."
# #         except Exception as e:
# #             logger.error(f"Gemini general QA error: {e}")
# #             return "‚ùå Kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y l√∫c n√†y. Vui l√≤ng th·ª≠ l·∫°i sau."
# #
# #     def get_suggested_questions(self) -> List[str]:
# #         # Danh s√°ch g·ª£i √Ω m·∫∑c ƒë·ªãnh cho giao di·ªán chat
# #         return [
# #             "Th∆∞ vi·ªán m·ªü c·ª≠a l√∫c m·∫•y gi·ªù?",
# #             "L√†m sao ƒë·ªÉ gia h·∫°n s√°ch?",
# #             "C√≥ s√°ch n√†o v·ªÅ tr√≠ tu·ªá nh√¢n t·∫°o kh√¥ng?",
# #             "Ph√≠ ph·∫°t tr·∫£ s√°ch tr·ªÖ l√† bao nhi√™u?",
# #         ]
# #
# #     def _is_book_related_query(self, question: str) -> bool:
# #         """
# #         Ki·ªÉm tra xem c√¢u h·ªèi c√≥ li√™n quan ƒë·∫øn vi·ªác t√¨m/h·ªèi v·ªÅ s√°ch kh√¥ng.
# #         D√πng ƒë·ªÉ quy·∫øt ƒë·ªãnh c√≥ n√™n d√πng cache s√°ch hay kh√¥ng.
# #         """
# #         q = question.lower()
# #         q = re.sub(r'[?.!,;:]', '', q)
# #
# #         book_keywords = [
# #             # T·ª´ kh√≥a s√°ch ti·∫øng Vi·ªát
# #             "s√°ch", "cu·ªën", "quy·ªÉn", "t√†i li·ªáu", "gi√°o tr√¨nh", "truy·ªán",
# #             "ti·ªÉu thuy·∫øt", "t√°c ph·∫©m", "ebook", "pdf",
# #             # T·ª´ kh√≥a s√°ch kh√¥ng d·∫•u
# #             "sach", "cuon", "quyen", "tai lieu", "giao trinh", "truyen",
# #             "tieu thuyet", "tac pham",
# #             # T·ª´ kh√≥a t√¨m ki·∫øm
# #             "t√¨m", "t√¨m ki·∫øm", "g·ª£i √Ω", "ƒë·ªÅ xu·∫•t", "cho t√¥i", "c√≥ kh√¥ng",
# #             "tim", "tim kiem", "goi y", "de xuat", "cho toi", "co khong",
# #             # Th·ªÉ lo·∫°i s√°ch
# #             "python", "java", "programming", "l·∫≠p tr√¨nh", "lap trinh",
# #             "machine learning", "ai", "deep learning", "data science",
# #             "to√°n", "vƒÉn", "l·ªãch s·ª≠", "ƒë·ªãa l√Ω", "v·∫≠t l√Ω", "h√≥a h·ªçc",
# #             "toan", "van", "lich su", "dia ly", "vat ly", "hoa hoc",
# #             # Ti·∫øng Anh
# #             "book", "novel", "textbook", "recommend", "find", "search"
# #         ]
# #
# #         return any(k in q for k in book_keywords)
# # =======
# #         # 2. Model Manager (replaces single client)
# #         self.model_manager = ModelManager(
# #             api_keys=GEMINI_API_KEYS,
# #             models=GEMINI_MODELS
# #         )
# #
# #         # 3. Session storage {session_id: ChatSession}
# #         self.sessions: Dict[str, ChatSession] = {}
# #
# #     def get_session(self, session_id: str) -> ChatSession:
# #         if session_id not in self.sessions:
# #             session = ChatSession(session_id)
# #             session.load()  # Try loading from disk
# #             self.sessions[session_id] = session
# #         return self.sessions[session_id]
# # >>>>>>> origin/main
# #
# #     # ==================================================
# #     # INTENT CLASSIFICATION
# #     # ==================================================
# #     def classify_intent(self, query: str, session: ChatSession) -> str:
# #         q = query.strip().lower()
# #
# #         # 1. Garbage check
# #         if len(q) < 2 or not re.search(r"[a-zA-Z√Ä-·ªπ0-9]", q):
# #             return "GARBAGE"
# #
# #         # 2. Greeting check
# #         greetings = ["hi", "hello", "ch√†o", "xin ch√†o", "hey", "b·∫°n ∆°i", "gi√∫p m√¨nh"]
# #         if q in greetings:
# #             return "GREETING"
# #
# #         # 3. Follow-up check
# #         if session.last_search_results:
# #             followup_keywords = [
# #                 "cu·ªën n√†y", "cu·ªën ƒë√≥", "cu·ªën th·ª©", "s√°ch n√†y", "s√°ch ƒë√≥",
# #                 "chi ti·∫øt", "n√≥ n√≥i v·ªÅ", "t√°c gi·∫£ l√† ai", "gi√° bao nhi√™u"
# #             ]
# #             if any(k in q for k in followup_keywords):
# #                 return "FOLLOWUP"
# #
# #             if re.search(r"(cu·ªën|s·ªë|quy·ªÉn)\s*\d+", q):
# #                 return "FOLLOWUP"
# #
# #         # 4. Default to SEARCH
# #         return "SEARCH"
# #
# #     # ==================================================
# #     # HANDLERS
# #     # ==================================================
# #     def answer_greeting(self) -> str:
# #         return "üëã Xin ch√†o! T√¥i l√† tr·ª£ l√Ω th∆∞ vi·ªán AI. T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n h√¥m nay? (T√¨m s√°ch, h·ªèi n·ªôi quy, v.v...)"
# #
# #     def answer_followup(self, question: str, session: ChatSession) -> str:
# #         """Tr·∫£ l·ªùi follow-up d·ª±a tr√™n last_search_results c·ªßa session"""
# #         q = question.lower()
# #
# #         # 1. Check for "all" / "summarize all"
# #         if any(k in q for k in ["t·∫•t c·∫£", "c·∫£ hai", "c·∫£ 2", "c·∫£ 3", "m·ªçi cu·ªën", "nh·ªØng cu·ªën n√†y", "c√°c cu·ªën", "hai cu·ªën", "2 cu·ªën", "ba cu·ªën", "3 cu·ªën"]):
# #             # Synthesize all books in context
# #             books_text = "\n".join([
# #                  f"{i}. {d['title']} ‚Äì {d['authors']}" for i, d in enumerate(session.last_search_results, 1)
# #             ])
# #             ctx = self._build_library_context()
# #             prompt = f"""{SYSTEM_PROMPT}\n{USER_PROMPT_TEMPLATE.format(question=question, books=books_text, **ctx)}"""
# #             return self._call_gemini(prompt)
# #
# #         # 2. Extract specific index (digits or text)
# #         idx = -1
# #
# #         # Mapping text to number
# #         text_nums = {
# #             "m·ªôt": 1, "hai": 2, "ba": 3, "b·ªën": 4, "nƒÉm": 5,
# #             "nh·∫•t": 1, "nh√¨": 2, "ƒë·∫ßu ti√™n": 1, "cu·ªëi c√πng": len(session.last_search_results)
# #         }
# #
# #         # Regex for text numbers
# #         text_pattern = r"(th·ª©|s·ªë|cu·ªën|quy·ªÉn)\s*(" + "|".join(text_nums.keys()) + r")"
# #         match_text = re.search(text_pattern, q)
# #
# #         # Regex for digits (flexible: "cu·ªën 1", "s·ªë 1", "1.")
# #         digit_pattern = r"(?:th·ª©|s·ªë|cu·ªën|quy·ªÉn|^)\s*(\d+)"
# #         match_digit = re.search(digit_pattern, q)
# #
# # <<<<<<< HEAD
# #     # ==================================================
# #     # üß† NH·∫¨N DI·ªÜN FOLLOW-UP QUESTION
# #     # ==================================================
# #     def is_followup_query(self, question: str) -> bool:
# #         """
# #         V√≠ d·ª•:
# #         - "Cu·ªën th·ª© th√¨ sao?"
# #         - "Cu·ªën n√†y ai vi·∫øt?"
# #         - "Trong s·ªë c√°c cu·ªën ƒë√≥, cu·ªën n√†o d·ªÖ h·ªçc nh·∫•t?"
# #         """
# #         if not self.last_docs:
# #             return False
# #
# #         q = question.lower()
# #         keywords = [
# #             "cu·ªën n√†y",
# #             "cu·ªën ƒë√≥",
# #             "cu·ªën th·ª©",
# #             "s√°ch n√†y",
# #             "s√°ch ƒë√≥",
# #             "trong s·ªë",
# #             "cu·ªën n√†o",
# #             "c√°i n√†o",
# #             "d·ªÖ h·ªçc",
# #             "t·ªët nh·∫•t",
# #             "ph√π h·ª£p",
# #             "n√™n ch·ªçn",
# #             "·ªü tr√™n",
# #             "v·ª´a r·ªìi",
# #             "trong danh s√°ch",
# #         ]
# #         return any(k in q for k in keywords)
# #
# #     def answer_followup(self, question: str) -> str:
# #         """
# #         Tr·∫£ l·ªùi follow-up d·ª±a tr√™n danh s√°ch s√°ch l·∫ßn tr∆∞·ªõc.
# #         - N·∫øu c√≥ ch·ªâ m·ª•c ("cu·ªën th·ª© 2") th√¨ tr·∫£ v·ªÅ s√°ch t∆∞∆°ng ·ª©ng.
# #         - N·∫øu l√† c√¢u ch·ªçn l·ªçc ("cu·ªën n√†o d·ªÖ h·ªçc nh·∫•t") th√¨ d√πng LLM t·ªïng h·ª£p tr√™n danh s√°ch hi·ªán c√≥.
# #         """
# #         if not self.last_docs:
# #             return "‚ùå T√¥i ch∆∞a c√≥ danh s√°ch s√°ch ƒë·ªÉ tham chi·∫øu."
# #
# #         q = question.lower()
# #         match = re.search(r"th·ª©\s*(\d+)", q)
# #         if match:
# #             idx = int(match.group(1)) - 1
# #             if 0 <= idx < len(self.last_docs):
# #                 b = self.last_docs[idx]
# #                 snippet = self._shorten_text(b.get("richtext") or b.get("snippet"))
# #                 return (
# #                     f"üìò **{b['title']}**\n"
# #                     f"- T√°c gi·∫£: {b['authors']}\n"
# #                     f"- NƒÉm xu·∫•t b·∫£n: {b['publish_year']}\n\n"
# #                     f"{snippet}"
# #                 )
# #             return "‚ùå Kh√¥ng t√¨m th·∫•y cu·ªën s√°ch b·∫°n y√™u c·∫ßu."
# #
# #         books_context = self._books_to_context(self.last_docs)
# #         prompt = FOLLOWUP_PROMPT_TEMPLATE.format(
# #             history=self._history_to_text(),
# #             previous_books=books_context,
# #             question=question
# #         )
# #         try:
# #             response = self.client.models.generate_content(
# #                 model=GEMINI_MODEL,
# #                 contents=prompt,
# #                 config=types.GenerateContentConfig(
# #                     temperature=min(TEMPERATURE, 0.5),
# #                     max_output_tokens=MAX_OUTPUT_TOKENS
# #                 )
# #             )
# #             return response.text.strip() if response and response.text else "‚ùå T√¥i ch∆∞a x√°c ƒë·ªãnh ƒë∆∞·ª£c c√¢u tr·∫£ l·ªùi."
# #         except Exception as e:
# #             logger.error(f"Gemini follow-up error: {e}")
# #             return "‚ùå Kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi follow-up."
# # =======
# #         if match_text:
# #             key = match_text.group(2)
# #             idx = text_nums.get(key, 0) - 1
# #         elif match_digit:
# #             idx = int(match_digit.group(1)) - 1
# #
# #         # 3. Return info if index valid
# #         if 0 <= idx < len(session.last_search_results):
# #             b = session.last_search_results[idx]
# #             # Use LLM description if simple info return
# #             return (
# #                 f"üìò **{b['title']}**\n"
# #                 f"- T√°c gi·∫£: {b['authors']}\n"
# #                 f"- NƒÉm xu·∫•t b·∫£n: {b['publish_year']}\n"
# #                 f"- M√£ s√°ch: {b['identifier']}\n\n"
# #                 f"üìù **N·ªôi dung:**\n{b.get('richtext','')[:1000]}..."
# #             )
# #
# #         # Fallback: Ask for clarification
# #         if session.last_search_results:
# #             return "B·∫°n mu·ªën h·ªèi v·ªÅ cu·ªën s√°ch s·ªë m·∫•y trong danh s√°ch tr√™n? (V√≠ d·ª•: 'cu·ªën s·ªë 1', 'quy·ªÉn ƒë·∫ßu ti√™n', 't·∫•t c·∫£')"
# #
# #         # Should not happen if detected as Followup
# #         return "‚ùå T√¥i kh√¥ng nh·ªõ ch√∫ng ta ƒëang n√≥i v·ªÅ cu·ªën n√†o. B·∫°n h√£y t√¨m ki·∫øm l·∫°i nh√©."
# # >>>>>>> origin/main
# #
# #     def needs_synthesis(self, question: str) -> bool:
# #         q = question.lower()
# #         return any(k in q for k in [
# #             "n√™n", "ph√π h·ª£p", "g·ª£i √Ω", "so s√°nh", "ƒë√°nh gi√°",
# #             "ph√¢n t√≠ch", "t·ªïng h·ª£p", "gi·∫£i th√≠ch", "v√¨ sao", "nh∆∞ th·∫ø n√†o"
# #         ])
# #
# #     def _build_library_context(self) -> dict:
# #         return {
# #             "opening_hours": LIBRARY_INFO["opening_hours"],
# #             "library_rules": "\n".join(f"- {r}" for r in LIBRARY_INFO["library_rules"]),
# #             "borrow_policy": "\n".join(f"- {k}: {v}" for k, v in LIBRARY_INFO["borrow_policy"].items()),
# #             "penalty_policy": "\n".join(f"- {k}: {v}" for k, v in LIBRARY_INFO["penalty_policy"].items()),
# #         }
# #
# #     # ==================================================
# #     # MAIN GENERATE FUNCTION
# #     # ==================================================
# #     def generate_answer(self, question: str, session_id: str = "default") -> str:
# #         session = self.get_session(session_id)
# #         session.add_message("user", question)
# #
# #         intent = self.classify_intent(question, session)
# #         logger.info(f"Session: {session_id} | Intent: {intent} | Query: {question}")
# #
# #         answer = ""
# #         if intent == "GARBAGE":
# #             answer = "‚ùå C√¢u h·ªèi kh√¥ng h·ª£p l·ªá ho·∫∑c qu√° ng·∫Øn."
# #         elif intent == "GREETING":
# #             answer = self.answer_greeting()
# #         elif intent == "FOLLOWUP":
# #             answer = self.answer_followup(question, session)
# #         else:
# #             if self.is_library_stats_query(question):
# #                 total = self.vector_db.get_collection_stats().get("count", 0)
# #                 answer = f"üìö Hi·ªán t·∫°i th∆∞ vi·ªán c√≥ **{total} cu·ªën s√°ch** trong h·ªá th·ªëng."
# #             elif self.is_library_info_query(question):
# #                 answer = self._generate_library_info_answer(question)
# #             else:
# #                 answer = self._perform_book_search(question, session)
# #
# #         session.add_message("model", answer)
# #         return answer
# #
# #     # ==================================================
# #     # SUB-HANDLERS
# #     # ==================================================
# #     def is_library_stats_query(self, q: str) -> bool:
# #         return any(k in q.lower() for k in ["bao nhi√™u s√°ch", "t·ªïng s·ªë s√°ch", "s·ªë l∆∞·ª£ng s√°ch"])
# #
# #     def is_library_info_query(self, q: str) -> bool:
# #         return any(k in q.lower() for k in ["m·ªü c·ª≠a", "quy ƒë·ªãnh", "m∆∞·ª£n s√°ch", "tr·∫£ s√°ch", "ph√≠ ph·∫°t"])
# #
# # <<<<<<< HEAD
# #         # ==================================================
# #         # Ô∏è‚É£ SMALLTALK / CH√ÄO H·ªéI (b·ªè qua cache ƒë·ªÉ tr√°nh hit s√°ch c≈©)
# #         # ==================================================
# #         if self.is_smalltalk(question):
# #             answer = self.smalltalk_answer(question)
# #             self._add_history("user", question)
# #             self._add_history("assistant", answer)
# #             return answer
# #
# #         # ==================================================
# #         # Ô∏è‚É£ QUERY MEMORY (CACHE C√ÇU H·ªéI C≈®)
# #         # ==================================================
# #         q_vec = self.embedder.embed_text(question, is_query=True)
# #         if q_vec:
# #             cached = self.vector_db.search_query_memory(
# #                 q_vec, threshold=QUERY_CACHE_THRESHOLD
# #             )
# #             # Skip cache n·∫øu cache answer l√† danh s√°ch s√°ch nh∆∞ng query kh√¥ng li√™n quan s√°ch
# # =======
# #     def _generate_library_info_answer(self, question: str) -> str:
# #         ctx = self._build_library_context()
# #         prompt = f"""{SYSTEM_PROMPT}\n{USER_PROMPT_TEMPLATE.format(question=question, books="(Kh√¥ng √°p d·ª•ng)", **ctx)}"""
# #         return self._call_gemini(prompt)
# #
# #     def _perform_book_search(self, question: str, session: ChatSession) -> str:
# #         q_vec = self.embedder.embed_text(question, is_query=True)
# #         if q_vec:
# #             cached = self.vector_db.search_query_memory(q_vec, threshold=QUERY_CACHE_THRESHOLD)
# # >>>>>>> origin/main
# #             if cached:
# #                 is_book_cache = "üìö Danh s√°ch s√°ch" in cached or "Danh s√°ch s√°ch li√™n quan" in cached
# #                 if is_book_cache and not self._is_book_related_query(question):
# #                     logger.info("‚ö†Ô∏è Query memory SKIP (cached books for non-book query)")
# #                 else:
# #                     logger.info("‚ö° Query memory HIT")
# #                     answer = f"‚ö° {cached}"
# #                     self._add_history("user", question)
# #                     self._add_history("assistant", answer)
# #                     return answer
# #
# #         raw_docs = self.search_engine.search(query=question, top_k=self.top_k * SEARCH_EXPAND_FACTOR)
# #         if not raw_docs: return self._gemini_fallback(question)
# #
# #         best_score = max(d.get("score", 0) for d in raw_docs)
# #         if best_score < SCORE_THRESHOLD: return self._gemini_fallback(question)
# #
# # <<<<<<< HEAD
# #             self.vector_db.add_query_memory(
# #                 question, q_vec, answer, qtype="stats"
# #             )
# #             self._add_history("user", question)
# #             self._add_history("assistant", answer)
# #             return answer
# #
# #         # ==================================================
# #         # Ô∏è‚É£ N·ªòI QUY / GI·ªú GI·∫§C
# #         # ==================================================
# #         if self.is_library_info_query(question):
# #             ctx = self._build_library_context()
# #             prompt = f"""{SYSTEM_PROMPT}
# #
# # L·ªãch s·ª≠ h·ªôi tho·∫°i g·∫ßn ƒë√¢y:
# # {self._history_to_text()}
# #
# # {USER_PROMPT_TEMPLATE.format(
# #     question=question,
# #     books="(Kh√¥ng √°p d·ª•ng)",
# #     **ctx
# # )}
# # """
# #             try:
# #                 response = self.client.models.generate_content(
# #                     model=GEMINI_MODEL,
# #                     contents=prompt,
# #                     config=types.GenerateContentConfig(
# #                         temperature=TEMPERATURE,
# #                         max_output_tokens=MAX_OUTPUT_TOKENS
# #                     )
# #                 )
# #                 answer = response.text.strip() if response and response.text else "‚ùå Kh√¥ng thÔøΩÔøΩÔøΩ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."
# #             except Exception as e:
# #                 logger.error(f"Gemini API error: {e}")
# #                 answer = "‚ùå Kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."
# #
# #             self.vector_db.add_query_memory(
# #                 question, q_vec, answer, qtype="library_info"
# #             )
# #             self._add_history("user", question)
# #             self._add_history("assistant", answer)
# #             return answer
# #
# #         # ==================================================
# #         # Ô∏è‚É£ FOLLOW-UP (KH√îNG CACHE)
# #         # ==================================================
# #         if self.is_followup_query(question):
# #             answer = self.answer_followup(question)
# #             self._add_history("user", question)
# #             self._add_history("assistant", answer)
# #             return answer
# #
# #         # ==================================================
# #         # Ô∏è‚É£ BOOK RAG PIPELINE
# #         # ==================================================
# #         raw_docs = self.search_engine.search(
# #             query=question,
# #             top_k=self.top_k * SEARCH_EXPAND_FACTOR
# #         )
# #
# #         # L·ªçc theo score
# #         docs = self.apply_score_threshold(raw_docs)
# #
# #         if docs:
# #             # L∆∞u l·∫°i ƒë·ªÉ d√πng cho follow-up
# #             self.last_docs = docs[:self.top_k]
# #             self.last_books_text = "\n".join(
# #                 f"{i}. {d['title']} ‚Äì {d['authors']} ({d['publish_year']})"
# #                 for i, d in enumerate(self.last_docs, 1)
# #             )
# #
# #             # Build danh s√°ch s√°ch
# #             book_lines = [
# #                 f"{i}. {d['title']} ‚Äì {d['authors']} ({d['publish_year']})"
# #                 for i, d in enumerate(self.last_docs, 1)
# #             ]
# #
# #             books_text = "\n".join(book_lines)
# #
# #             # ==================================================
# #             # .Ô∏è‚É£ CH·ªà LIST, KH√îNG T·ªîNG H·ª¢P
# #             # ==================================================
# #             if not self.needs_synthesis(question):
# #                 answer = f"üìö Danh s√°ch s√°ch li√™n quan\n\n{books_text}"
# #
# #                 self.vector_db.add_query_memory(
# #                     question, q_vec, answer, qtype="rag_list"
# #                 )
# #                 self._add_history("user", question)
# #                 self._add_history("assistant", answer)
# #                 return answer
# #
# #             # ==================================================
# #             # .Ô∏è‚É£ C√ì G·ªåI LLM ƒê·ªÇ T·ªîNG H·ª¢P
# #             # ==================================================
# #             ctx = self._build_library_context()
# #
# #             prompt = f"""{SYSTEM_PROMPT}
# #
# # L·ªãch s·ª≠ h·ªôi tho·∫°i g·∫ßn ƒë√¢y:
# # {self._history_to_text()}
# #
# # {USER_PROMPT_TEMPLATE.format(
# #     question=question,
# #     books=books_text,
# #     **ctx
# # )}
# # """
# #             try:
# #                 response = self.client.models.generate_content(
# #                     model=GEMINI_MODEL,
# #                     contents=prompt,
# #                     config=types.GenerateContentConfig(
# #                         temperature=TEMPERATURE,
# #                         max_output_tokens=MAX_OUTPUT_TOKENS
# #                     )
# #                 )
# #                 synthesis = response.text.strip() if response and response.text else "‚ùå Kh√¥ng th·ªÉ t·ªïng h·ª£p th√¥ng tin."
# #             except Exception as e:
# #                 logger.error(f"Gemini API error: {e}")
# #                 synthesis = "‚ùå Kh√¥ng th·ªÉ t·ªïng h·ª£p th√¥ng tin."
# #
# #             answer = f"""üìö Danh s√°ch s√°ch li√™n quan
# #
# # {books_text}
# #
# # üìù T·ªïng h·ª£p
# # {synthesis}
# # """
# #             self.vector_db.add_query_memory(
# #                 question, q_vec, answer, qtype="rag_synthesis"
# #             )
# #             self._add_history("user", question)
# #             self._add_history("assistant", answer)
# #             return answer
# #
# #         # ==================================================
# #         # Ô∏è‚É£ FALLBACK: KH√îNG C√ì DATA ‚Üí D√ôNG LLM T·ªîNG QU√ÅT
# #         # ==================================================
# #         answer = self.general_llm_answer(question)
# #         self._add_history("user", question)
# #         self._add_history("assistant", answer)
# # =======
# #         docs = raw_docs[:self.top_k]
# #
# #         session.last_search_results = docs
# #         session.save()
# #
# #         book_lines = [
# #             f"{i}. {d['title']} ‚Äì {d['authors']} ({d['publish_year']})"
# #             for i, d in enumerate(docs, 1)
# #         ]
# #         books_text = "\n".join(book_lines)
# #
# #         if not self.needs_synthesis(question):
# #             answer = f"üìö Danh s√°ch s√°ch li√™n quan:\n\n{books_text}"
# #             if q_vec: self.vector_db.add_query_memory(question, q_vec, answer, qtype="rag_list")
# #             return answer
# #
# #         ctx = self._build_library_context()
# #         prompt = f"""{SYSTEM_PROMPT}\n{USER_PROMPT_TEMPLATE.format(question=question, books=books_text, **ctx)}"""
# #
# #         synthesis = self._call_gemini(prompt)
# #         answer = f"üìö Danh s√°ch s√°ch li√™n quan:\n\n{books_text}\n\nüìù T·ªïng h·ª£p:\n{synthesis}"
# #
# #         if q_vec: self.vector_db.add_query_memory(question, q_vec, answer, qtype="rag_synthesis")
# # >>>>>>> origin/main
# #         return answer
# #
# #     def _gemini_fallback(self, question: str) -> str:
# #         prompt = f"""B·∫°n l√† tr·ª£ l√Ω th∆∞ vi·ªán AI. Th∆∞ vi·ªán KH√îNG c√≥ d·ªØ li·ªáu cho c√¢u h·ªèi: "{question}". Y√™u c·∫ßu: N√≥i r√µ kh√¥ng c√≥ d·ªØ li·ªáu, KH√îNG b·ªãa t√™n s√°ch."""
# #         return self._call_gemini(prompt)
# #
# #     def _call_gemini(self, prompt: str) -> str:
# #         """Call Gemini via ModelManager (handles rotation & rate limits)"""
# #         try:
# #             # New call using ModelManager
# #             result = self.model_manager.generate_content(
# #                 prompt=prompt,
# #                 temperature=TEMPERATURE,
# #                 max_tokens=MAX_OUTPUT_TOKENS
# #             )
# #             return result if result else "‚ùå Xin l·ªói, kh√¥ng c√≥ ph·∫£n h·ªìi."
# #         except Exception as e:
# #             logger.error(f"Gemini API error: {e}")
# #             return "‚ùå H·ªá th·ªëng ƒëang b·∫≠n ho·∫∑c g·∫∑p s·ª± c·ªë k·∫øt n·ªëi."
# =======
# import os
# import re
# import json
# import logging
# from google import genai
# from google.genai import types
# from typing import List, Dict
#
# from src.search_engine import SearchEngine
# from src.rag.prompt import (
#     SYSTEM_PROMPT,
#     USER_PROMPT_TEMPLATE,
#     LIBRARY_INFO,
#     FOLLOWUP_PROMPT_TEMPLATE,
#     SMALLTALK_PROMPT_TEMPLATE,
#     GENERAL_QA_PROMPT_TEMPLATE
# )
# from config.rag_config import (
#     GEMINI_API_KEY,
#     GEMINI_MODEL,
#     DEFAULT_TOP_K,
#     SCORE_THRESHOLD,
#     MIN_QUERY_LENGTH,
#     TEMPERATURE,
#     MAX_OUTPUT_TOKENS,
#     QUERY_CACHE_THRESHOLD,
#     SEARCH_EXPAND_FACTOR
# )
#
#
# # Logger cho module RAG
# logger = logging.getLogger("RAGEngine")
#
#
# class RAGEngine:
#     """
#     ========================================================
#     ü§ñ RAGEngine
#     --------------------------------------------------------
#     Ch·ª©c nƒÉng:
#     - Nh·∫≠n c√¢u h·ªèi ng∆∞·ªùi d√πng
#     - Ph√¢n lo·∫°i: th·ªëng k√™ / n·ªôi quy / follow-up / t√¨m s√°ch / t·ªïng h·ª£p / fallback
#     - Search vector DB
#     - Build prompt cho Gemini
#     - Cache l·∫°i c√¢u h·ªèi ƒë√£ h·ªèi (Query Memory)
#     ========================================================
#     """
#
#     def __init__(self, top_k: int = DEFAULT_TOP_K):
#         # ===============================
#         # Ô∏è‚É£ SEARCH ENGINE (Vector DB + Embedder)
#         # ===============================
#         self.search_engine = SearchEngine()
#         self.embedder = self.search_engine.embedder
#         self.vector_db = self.search_engine.vector_db
#         self.top_k = top_k
#
#         # Initialize Gemini client
#         self.client = genai.Client(api_key=GEMINI_API_KEY)
#         self.last_docs = []  # For follow-up queries
#         self.last_books_text = ""
#         self.history: list[dict] = []  # [{role: user/assistant, content: str}]
#
#     def _add_history(self, role: str, content: str, max_turns: int = 6):
#         # L∆∞u ng·∫Øn g·ªçn l·ªãch s·ª≠ ƒë·ªÉ LLM c√≥ ng·ªØ c·∫£nh follow-up
#         self.history.append({"role": role, "content": content})
#         if len(self.history) > max_turns * 2:
#             self.history = self.history[-max_turns * 2 :]
#
#     def _history_to_text(self) -> str:
#         if not self.history:
#             return "(ch∆∞a c√≥ l·ªãch s·ª≠)"
#         lines = []
#         for h in self.history[-8:]:
#             prefix = "Ng∆∞·ªùi d√πng" if h["role"] == "user" else "Tr·ª£ l√Ω"
#             lines.append(f"{prefix}: {h['content']}")
#         return "\n".join(lines)
#
#     def _shorten_text(self, text: str | None, max_len: int = 480) -> str:
#         if not text:
#             return "(kh√¥ng c√≥ m√¥ t·∫£)"
#         text = text.strip()
#         return text[:max_len] + "‚Ä¶" if len(text) > max_len else text
#
#     def _books_to_context(self, docs: List[Dict]) -> str:
#         lines = []
#         for i, b in enumerate(docs, 1):
#             snippet = self._shorten_text(b.get("richtext") or b.get("snippet"))
#             lines.append(
#                 f"{i}. {b.get('title')} ‚Äì {b.get('authors')} ({b.get('publish_year')})\n{snippet}"
#             )
#         return "\n\n".join(lines)
#
#     def is_smalltalk(self, question: str) -> bool:
#         """
#         Nh·∫≠n di·ªán c√¢u h·ªèi smalltalk / ch√†o h·ªèi.
#         H·ªó tr·ª£ c·∫£ ti·∫øng Vi·ªát c√≥ d·∫•u v√† kh√¥ng d·∫•u.
#         """
#         q = question.lower().strip()
#
#         # Lo·∫°i b·ªè d·∫•u c√¢u
#         q = re.sub(r'[?.!,;:]', '', q)
#
#         smalltalk_keywords = [
#             # Ch√†o h·ªèi c√≥ d·∫•u
#             "xin ch√†o", "ch√†o b·∫°n", "ch√†o", "ch√†o bu·ªïi s√°ng", "ch√†o bu·ªïi t·ªëi",
#             # Ch√†o h·ªèi kh√¥ng d·∫•u
#             "xin chao", "chao ban", "chao", "chao buoi sang", "chao buoi toi",
#             # Ti·∫øng Anh
#             "hello", "hi", "hey", "good morning", "good afternoon", "good evening",
#             # C·∫£m ∆°n c√≥ d·∫•u
#             "c·∫£m ∆°n", "c√°m ∆°n", "c·∫£m ∆°n b·∫°n", "c√°m ∆°n b·∫°n",
#             # C·∫£m ∆°n kh√¥ng d·∫•u
#             "cam on", "cam on ban",
#             # Ti·∫øng Anh
#             "thank", "thanks", "thank you", "tks", "ty",
#             # T·∫°m bi·ªát c√≥ d·∫•u
#             "t·∫°m bi·ªát", "h·∫πn g·∫∑p l·∫°i", "g·∫∑p l·∫°i sau",
#             # T·∫°m bi·ªát kh√¥ng d·∫•u
#             "tam biet", "hen gap lai", "gap lai sau",
#             # Ti·∫øng Anh
#             "bye", "goodbye", "see you", "see ya",
#             # H·ªèi thƒÉm c√≥ d·∫•u
#             "b·∫°n l√† ai", "t√™n g√¨", "kh·ªèe kh√¥ng", "b·∫°n ·ªïn kh√¥ng", "b·∫°n c√≥ kh·ªèe kh√¥ng",
#             # H·ªèi thƒÉm kh√¥ng d·∫•u
#             "ban la ai", "ten gi", "khoe khong", "ban on khong", "ban co khoe khong",
#             # H·ªèi thƒÉm ti·∫øng Anh
#             "how are you", "what's up", "who are you", "what is your name",
#             # C√°c c√¢u ƒë∆°n gi·∫£n
#             "alo", "yo", "hii", "hiii", "helloo", "helo"
#         ]
#
#         # Ki·ªÉm tra exact match tr∆∞·ªõc
#         if q in smalltalk_keywords:
#             return True
#
#         # Ki·ªÉm tra contains
#         return any(k in q for k in smalltalk_keywords)
#
#     def smalltalk_answer(self, question: str) -> str:
#         """
#         D√πng Gemini tr·∫£ l·ªùi smalltalk m·ªôt c√°ch th√¥ng minh v√† t·ª± nhi√™n.
#         """
#         prompt = SMALLTALK_PROMPT_TEMPLATE.format(
#             history=self._history_to_text(),
#             question=question
#         )
#
#         try:
#             response = self.client.models.generate_content(
#                 model=GEMINI_MODEL,
#                 contents=prompt,
#                 config=types.GenerateContentConfig(
#                     temperature=0.7,  # Cao h∆°n ƒë·ªÉ tr·∫£ l·ªùi t·ª± nhi√™n h∆°n
#                     max_output_tokens=150  # Ng·∫Øn g·ªçn
#                 )
#             )
#             return response.text.strip() if response and response.text else "üëã Ch√†o b·∫°n! M√¨nh c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?"
#         except Exception as e:
#             logger.error(f"Gemini smalltalk error: {e}")
#             # Fallback n·∫øu API l·ªói
#             return "üëã Ch√†o b·∫°n! M√¨nh l√† tr·ª£ l√Ω th∆∞ vi·ªán AI. B·∫°n c·∫ßn t√¨m s√°ch g√¨ h√¥m nay?"
#
#     def general_llm_answer(self, question: str) -> str:
#         """
#         D√πng Gemini tr·∫£ l·ªùi c√°c c√¢u h·ªèi t·ªïng qu√°t khi kh√¥ng t√¨m th·∫•y s√°ch ph√π h·ª£p.
#         """
#         prompt = GENERAL_QA_PROMPT_TEMPLATE.format(
#             history=self._history_to_text(),
#             question=question
#         )
#
#         try:
#             response = self.client.models.generate_content(
#                 model=GEMINI_MODEL,
#                 contents=prompt,
#                 config=types.GenerateContentConfig(
#                     temperature=0.5,
#                     max_output_tokens=MAX_OUTPUT_TOKENS
#                 )
#             )
#             return response.text.strip() if response and response.text else "‚ùå T√¥i ch∆∞a c√≥ c√¢u tr·∫£ l·ªùi ph√π h·ª£p."
#         except Exception as e:
#             logger.error(f"Gemini general QA error: {e}")
#             return "‚ùå Kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y l√∫c n√†y. Vui l√≤ng th·ª≠ l·∫°i sau."
#
#     def get_suggested_questions(self) -> List[str]:
#         # Danh s√°ch g·ª£i √Ω m·∫∑c ƒë·ªãnh cho giao di·ªán chat
#         return [
#             "Th∆∞ vi·ªán m·ªü c·ª≠a l√∫c m·∫•y gi·ªù?",
#             "L√†m sao ƒë·ªÉ gia h·∫°n s√°ch?",
#             "C√≥ s√°ch n√†o v·ªÅ tr√≠ tu·ªá nh√¢n t·∫°o kh√¥ng?",
#             "Ph√≠ ph·∫°t tr·∫£ s√°ch tr·ªÖ l√† bao nhi√™u?",
#         ]
#
#     def _is_book_related_query(self, question: str) -> bool:
#         """
#         Ki·ªÉm tra xem c√¢u h·ªèi c√≥ li√™n quan ƒë·∫øn vi·ªác t√¨m/h·ªèi v·ªÅ s√°ch kh√¥ng.
#         D√πng ƒë·ªÉ quy·∫øt ƒë·ªãnh c√≥ n√™n d√πng cache s√°ch hay kh√¥ng.
#         """
#         q = question.lower()
#         q = re.sub(r'[?.!,;:]', '', q)
#
#         book_keywords = [
#             # T·ª´ kh√≥a s√°ch ti·∫øng Vi·ªát
#             "s√°ch", "cu·ªën", "quy·ªÉn", "t√†i li·ªáu", "gi√°o tr√¨nh", "truy·ªán",
#             "ti·ªÉu thuy·∫øt", "t√°c ph·∫©m", "ebook", "pdf",
#             # T·ª´ kh√≥a s√°ch kh√¥ng d·∫•u
#             "sach", "cuon", "quyen", "tai lieu", "giao trinh", "truyen",
#             "tieu thuyet", "tac pham",
#             # T·ª´ kh√≥a t√¨m ki·∫øm
#             "t√¨m", "t√¨m ki·∫øm", "g·ª£i √Ω", "ƒë·ªÅ xu·∫•t", "cho t√¥i", "c√≥ kh√¥ng",
#             "tim", "tim kiem", "goi y", "de xuat", "cho toi", "co khong",
#             # Th·ªÉ lo·∫°i s√°ch
#             "python", "java", "programming", "l·∫≠p tr√¨nh", "lap trinh",
#             "machine learning", "ai", "deep learning", "data science",
#             "to√°n", "vƒÉn", "l·ªãch s·ª≠", "ƒë·ªãa l√Ω", "v·∫≠t l√Ω", "h√≥a h·ªçc",
#             "toan", "van", "lich su", "dia ly", "vat ly", "hoa hoc",
#             # Ti·∫øng Anh
#             "book", "novel", "textbook", "recommend", "find", "search"
#         ]
#
#         return any(k in q for k in book_keywords)
#
#     # ==================================================
#     # FILTER GARBAGE QUERIES
#     # ==================================================
#     def is_garbage_query(self, query: str) -> bool:
#         """
#         L·ªçc c√°c c√¢u h·ªèi r√°c:
#         - R·ªóng
#         - Qu√° ng·∫Øn
#         - To√†n s·ªë
#         - Kh√¥ng ch·ª©a ch·ªØ c√°i
#         """
#         if not query or not query.strip():
#             return True
#
#         q = query.strip().lower()
#
#         if len(q) < MIN_QUERY_LENGTH:
#             return True
#         if q.isdigit():
#             return True
#
#         # Kh√¥ng c√≥ ch·ªØ c√°i (k·ªÉ c·∫£ ti·∫øng Vi·ªát)
#         if not re.search(r"[a-zA-Z√Ä-·ªπ]", q):
#             return True
#
#         return False
#
#     # ==================================================
#     # üìä NH·∫¨N DI·ªÜN C√ÇU H·ªéI TH·ªêNG K√ä
#     # ==================================================
#     def is_library_stats_query(self, question: str) -> bool:
#         """
#         V√≠ d·ª•:
#         - "Th∆∞ vi·ªán c√≥ bao nhi√™u cu·ªën s√°ch?"
#         - "T·ªïng s·ªë s√°ch l√† bao nhi√™u?"
#         """
#         q = question.lower()
#         return any(k in q for k in [
#             "bao nhi√™u s√°ch",
#             "bao nhi√™u cu·ªën",
#             "t·ªïng s·ªë s√°ch",
#             "s·ªë l∆∞·ª£ng s√°ch",
#             "th∆∞ vi·ªán c√≥ bao nhi√™u"
#         ])
#
#     # ==================================================
#     # üèõÔ∏è NH·∫¨N DI·ªÜN C√ÇU H·ªéI N·ªòI QUY / GI·ªú GI·∫§C
#     # ==================================================
#     def is_library_info_query(self, question: str) -> bool:
#         """
#         V√≠ d·ª•:
#         - M·∫•y gi·ªù m·ªü c·ª≠a?
#         - Quy ƒë·ªãnh m∆∞·ª£n s√°ch?
#         - Ph√≠ ph·∫°t th·∫ø n√†o?
#         """
#         q = question.lower()
#         return any(k in q for k in [
#             "m·ªü c·ª≠a",
#             "ƒë√≥ng c·ª≠a",
#             "gi·ªù m·ªü",
#             "gi·ªù ƒë√≥ng",
#             "gi·ªù l√†m vi·ªác",
#             "n·ªôi quy",
#             "quy ƒë·ªãnh",
#             "m∆∞·ª£n s√°ch",
#             "tr·∫£ s√°ch",
#             "gia h·∫°n",
#             "ph√≠ ph·∫°t"
#         ])
#
#     # ==================================================
#     # üß† NH·∫¨N DI·ªÜN FOLLOW-UP QUESTION
#     # ==================================================
#     def is_followup_query(self, question: str) -> bool:
#         """
#         V√≠ d·ª•:
#         - "Cu·ªën th·ª© th√¨ sao?"
#         - "Cu·ªën n√†y ai vi·∫øt?"
#         - "Trong s·ªë c√°c cu·ªën ƒë√≥, cu·ªën n√†o d·ªÖ h·ªçc nh·∫•t?"
#         """
#         if not self.last_docs:
#             return False
#
#         q = question.lower()
#         keywords = [
#             "cu·ªën n√†y",
#             "cu·ªën ƒë√≥",
#             "cu·ªën th·ª©",
#             "s√°ch n√†y",
#             "s√°ch ƒë√≥",
#             "trong s·ªë",
#             "cu·ªën n√†o",
#             "c√°i n√†o",
#             "d·ªÖ h·ªçc",
#             "t·ªët nh·∫•t",
#             "ph√π h·ª£p",
#             "n√™n ch·ªçn",
#             "·ªü tr√™n",
#             "v·ª´a r·ªìi",
#             "trong danh s√°ch",
#         ]
#         return any(k in q for k in keywords)
#
#     def answer_followup(self, question: str) -> str:
#         """
#         Tr·∫£ l·ªùi follow-up d·ª±a tr√™n danh s√°ch s√°ch l·∫ßn tr∆∞·ªõc.
#         - N·∫øu c√≥ ch·ªâ m·ª•c ("cu·ªën th·ª© 2") th√¨ tr·∫£ v·ªÅ s√°ch t∆∞∆°ng ·ª©ng.
#         - N·∫øu l√† c√¢u ch·ªçn l·ªçc ("cu·ªën n√†o d·ªÖ h·ªçc nh·∫•t") th√¨ d√πng LLM t·ªïng h·ª£p tr√™n danh s√°ch hi·ªán c√≥.
#         """
#         if not self.last_docs:
#             return "‚ùå T√¥i ch∆∞a c√≥ danh s√°ch s√°ch ƒë·ªÉ tham chi·∫øu."
#
#         q = question.lower()
#         match = re.search(r"th·ª©\s*(\d+)", q)
#         if match:
#             idx = int(match.group(1)) - 1
#             if 0 <= idx < len(self.last_docs):
#                 b = self.last_docs[idx]
#                 snippet = self._shorten_text(b.get("richtext") or b.get("snippet"))
#                 return (
#                     f"üìò **{b['title']}**\n"
#                     f"- T√°c gi·∫£: {b['authors']}\n"
#                     f"- NƒÉm xu·∫•t b·∫£n: {b['publish_year']}\n\n"
#                     f"{snippet}"
#                 )
#             return "‚ùå Kh√¥ng t√¨m th·∫•y cu·ªën s√°ch b·∫°n y√™u c·∫ßu."
#
#         books_context = self._books_to_context(self.last_docs)
#         prompt = FOLLOWUP_PROMPT_TEMPLATE.format(
#             history=self._history_to_text(),
#             previous_books=books_context,
#             question=question
#         )
#         try:
#             response = self.client.models.generate_content(
#                 model=GEMINI_MODEL,
#                 contents=prompt,
#                 config=types.GenerateContentConfig(
#                     temperature=min(TEMPERATURE, 0.5),
#                     max_output_tokens=MAX_OUTPUT_TOKENS
#                 )
#             )
#             return response.text.strip() if response and response.text else "‚ùå T√¥i ch∆∞a x√°c ƒë·ªãnh ƒë∆∞·ª£c c√¢u tr·∫£ l·ªùi."
#         except Exception as e:
#             logger.error(f"Gemini follow-up error: {e}")
#             return "‚ùå Kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi follow-up."
#
#     # ==================================================
#     # üß† C√ì C·∫¶N G·ªåI LLM ƒê·ªÇ T·ªîNG H·ª¢P KH√îNG?
#     # ==================================================
#     def needs_synthesis(self, question: str) -> bool:
#         """
#         N·∫øu ch·ªâ h·ªèi:
#         - "S√°ch v·ªÅ AI" ‚Üí ch·ªâ list
#
#         N·∫øu h·ªèi:
#         - "N√™n ƒë·ªçc s√°ch n√†o?"
#         - "So s√°nh gi√∫p t√¥i"
#         ‚Üí c·∫ßn LLM t·ªïng h·ª£p
#         """
#         q = question.lower()
#         return any(k in q for k in [
#             "n√™n",
#             "ph√π h·ª£p",
#             "g·ª£i √Ω",
#             "so s√°nh",
#             "ƒë√°nh gi√°",
#             "ph√¢n t√≠ch",
#             "t·ªïng h·ª£p",
#             "gi·∫£i th√≠ch",
#             "v√¨ sao",
#             "nh∆∞ th·∫ø n√†o"
#         ])
#
#     # ==================================================
#     # üéØ L·ªåC THEO SCORE
#     # ==================================================
#     def apply_score_threshold(self, docs):
#         """
#         N·∫øu document t·ªët nh·∫•t < threshold ‚Üí coi nh∆∞ kh√¥ng c√≥ k·∫øt qu·∫£
#         """
#         if not docs:
#             return []
#
#         best = max(d.get("score", 0) for d in docs)
#         return docs if best >= SCORE_THRESHOLD else []
#
#     # ==================================================
#     # üèõÔ∏è BUILD CONTEXT N·ªòI QUY TH∆Ø VI·ªÜN
#     # ==================================================
#     def _build_library_context(self) -> dict:
#         """
#         Convert LIBRARY_INFO th√†nh text cho prompt
#         """
#         return {
#             "opening_hours": LIBRARY_INFO["opening_hours"],
#             "library_rules": "\n".join(f"- {r}" for r in LIBRARY_INFO["library_rules"]),
#             "borrow_policy": "\n".join(
#                 f"- {k}: {v}" for k, v in LIBRARY_INFO["borrow_policy"].items()
#             ),
#             "penalty_policy": "\n".join(
#                 f"- {k}: {v}" for k, v in LIBRARY_INFO["penalty_policy"].items()
#             ),
#         }
#
#     # ==================================================
#     # ü§ñ FALLBACK KHI KH√îNG C√ì DATA
#     # ==================================================
#     def gemini_fallback(self, question: str) -> str:
#         """
#         G·ªçi Gemini tr·∫£ l·ªùi chung chung nh∆∞ng:
#         - Ph·∫£i n√≥i r√µ l√† th∆∞ vi·ªán kh√¥ng c√≥ d·ªØ li·ªáu
#         - Kh√¥ng ƒë∆∞·ª£c b·ªãa s√°ch
#         """
#         prompt = f"""
# B·∫°n l√† tr·ª£ l√Ω th∆∞ vi·ªán AI.
#
# Th∆∞ vi·ªán KH√îNG c√≥ d·ªØ li·ªáu ph√π h·ª£p cho c√¢u h·ªèi:
# "{question}"
#
# Y√™u c·∫ßu:
# - N√≥i r√µ kh√¥ng c√≥ d·ªØ li·ªáu
# - KH√îNG b·ªãa t√™n s√°ch
# """
#         try:
#             response = self.client.models.generate_content(
#                 model=GEMINI_MODEL,
#                 contents=prompt,
#                 config=types.GenerateContentConfig(
#                     temperature=TEMPERATURE,
#                     max_output_tokens=MAX_OUTPUT_TOKENS
#                 )
#             )
#             return response.text.strip() if response and response.text else "‚ùå Xin l·ªói, t√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."
#         except Exception as e:
#             logger.error(f"Gemini fallback error: {e}")
#             return "‚ùå Xin l·ªói, th∆∞ vi·ªán kh√¥ng c√≥ th√¥ng tin ph√π h·ª£p v·ªõi c√¢u h·ªèi c·ªßa b·∫°n."
#
#     # ==================================================
#     # GENERATE ANSWER
#     # ==================================================
#     def generate_answer(self, question: str) -> str:
#
#         # ==================================================
#         # Ô∏è‚É£ CH·∫∂N C√ÇU H·ªéI R√ÅC
#         # ==================================================
#         if self.is_garbage_query(question):
#             return "‚ùå C√¢u h·ªèi kh√¥ng h·ª£p l·ªá ho·∫∑c qu√° ng·∫Øn."
#
#         # ==================================================
#         # Ô∏è‚É£ SMALLTALK / CH√ÄO H·ªéI (b·ªè qua cache ƒë·ªÉ tr√°nh hit s√°ch c≈©)
#         # ==================================================
#         if self.is_smalltalk(question):
#             answer = self.smalltalk_answer(question)
#             self._add_history("user", question)
#             self._add_history("assistant", answer)
#             return answer
#
#         # ==================================================
#         # Ô∏è‚É£ QUERY MEMORY (CACHE C√ÇU H·ªéI C≈®)
#         # ==================================================
#         q_vec = self.embedder.embed_text(question, is_query=True)
#         if q_vec:
#             cached = self.vector_db.search_query_memory(
#                 q_vec, threshold=QUERY_CACHE_THRESHOLD
#             )
#             # Skip cache n·∫øu cache answer l√† danh s√°ch s√°ch nh∆∞ng query kh√¥ng li√™n quan s√°ch
#             if cached:
#                 is_book_cache = "üìö Danh s√°ch s√°ch" in cached or "Danh s√°ch s√°ch li√™n quan" in cached
#                 if is_book_cache and not self._is_book_related_query(question):
#                     logger.info("‚ö†Ô∏è Query memory SKIP (cached books for non-book query)")
#                 else:
#                     logger.info("‚ö° Query memory HIT")
#                     answer = f"‚ö° {cached}"
#                     self._add_history("user", question)
#                     self._add_history("assistant", answer)
#                     return answer
#
#         # ==================================================
#         # Ô∏è‚É£ TH·ªêNG K√ä
#         # ==================================================
#         if self.is_library_stats_query(question):
#             total = self.vector_db.get_collection_stats().get("count", 0)
#             answer = f"üìö Hi·ªán t·∫°i th∆∞ vi·ªán c√≥ **{total} cu·ªën s√°ch** trong h·ªá th·ªëng."
#
#             self.vector_db.add_query_memory(
#                 question, q_vec, answer, qtype="stats"
#             )
#             self._add_history("user", question)
#             self._add_history("assistant", answer)
#             return answer
#
#         # ==================================================
#         # Ô∏è‚É£ N·ªòI QUY / GI·ªú GI·∫§C
#         # ==================================================
#         if self.is_library_info_query(question):
#             ctx = self._build_library_context()
#             prompt = f"""{SYSTEM_PROMPT}
#
# L·ªãch s·ª≠ h·ªôi tho·∫°i g·∫ßn ƒë√¢y:
# {self._history_to_text()}
#
# {USER_PROMPT_TEMPLATE.format(
#     question=question,
#     books="(Kh√¥ng √°p d·ª•ng)",
#     **ctx
# )}
# """
#             try:
#                 response = self.client.models.generate_content(
#                     model=GEMINI_MODEL,
#                     contents=prompt,
#                     config=types.GenerateContentConfig(
#                         temperature=TEMPERATURE,
#                         max_output_tokens=MAX_OUTPUT_TOKENS
#                     )
#                 )
#                 answer = response.text.strip() if response and response.text else "‚ùå Kh√¥ng thÔøΩÔøΩÔøΩ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."
#             except Exception as e:
#                 logger.error(f"Gemini API error: {e}")
#                 answer = "‚ùå Kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."
#
#             self.vector_db.add_query_memory(
#                 question, q_vec, answer, qtype="library_info"
#             )
#             self._add_history("user", question)
#             self._add_history("assistant", answer)
#             return answer
#
#         # ==================================================
#         # Ô∏è‚É£ FOLLOW-UP (KH√îNG CACHE)
#         # ==================================================
#         if self.is_followup_query(question):
#             answer = self.answer_followup(question)
#             self._add_history("user", question)
#             self._add_history("assistant", answer)
#             return answer
#
#         # ==================================================
#         # Ô∏è‚É£ BOOK RAG PIPELINE
#         # ==================================================
#         raw_docs = self.search_engine.search(
#             query=question,
#             top_k=self.top_k * SEARCH_EXPAND_FACTOR
#         )
#
#         # L·ªçc theo score
#         docs = self.apply_score_threshold(raw_docs)
#
#         if docs:
#             # L∆∞u l·∫°i ƒë·ªÉ d√πng cho follow-up
#             self.last_docs = docs[:self.top_k]
#             self.last_books_text = "\n".join(
#                 f"{i}. {d['title']} ‚Äì {d['authors']} ({d['publish_year']})"
#                 for i, d in enumerate(self.last_docs, 1)
#             )
#
#             # Build danh s√°ch s√°ch
#             book_lines = [
#                 f"{i}. {d['title']} ‚Äì {d['authors']} ({d['publish_year']})"
#                 for i, d in enumerate(self.last_docs, 1)
#             ]
#
#             books_text = "\n".join(book_lines)
#
#             # ==================================================
#             # .Ô∏è‚É£ CH·ªà LIST, KH√îNG T·ªîNG H·ª¢P
#             # ==================================================
#             if not self.needs_synthesis(question):
#                 answer = f"üìö Danh s√°ch s√°ch li√™n quan\n\n{books_text}"
#
#                 self.vector_db.add_query_memory(
#                     question, q_vec, answer, qtype="rag_list"
#                 )
#                 self._add_history("user", question)
#                 self._add_history("assistant", answer)
#                 return answer
#
#             # ==================================================
#             # .Ô∏è‚É£ C√ì G·ªåI LLM ƒê·ªÇ T·ªîNG H·ª¢P
#             # ==================================================
#             ctx = self._build_library_context()
#
#             prompt = f"""{SYSTEM_PROMPT}
#
# L·ªãch s·ª≠ h·ªôi tho·∫°i g·∫ßn ƒë√¢y:
# {self._history_to_text()}
#
# {USER_PROMPT_TEMPLATE.format(
#     question=question,
#     books=books_text,
#     **ctx
# )}
# """
#             try:
#                 response = self.client.models.generate_content(
#                     model=GEMINI_MODEL,
#                     contents=prompt,
#                     config=types.GenerateContentConfig(
#                         temperature=TEMPERATURE,
#                         max_output_tokens=MAX_OUTPUT_TOKENS
#                     )
#                 )
#                 synthesis = response.text.strip() if response and response.text else "‚ùå Kh√¥ng th·ªÉ t·ªïng h·ª£p th√¥ng tin."
#             except Exception as e:
#                 logger.error(f"Gemini API error: {e}")
#                 synthesis = "‚ùå Kh√¥ng th·ªÉ t·ªïng h·ª£p th√¥ng tin."
#
#             answer = f"""üìö Danh s√°ch s√°ch li√™n quan
#
# {books_text}
#
# üìù T·ªïng h·ª£p
# {synthesis}
# """
#             self.vector_db.add_query_memory(
#                 question, q_vec, answer, qtype="rag_synthesis"
#             )
#             self._add_history("user", question)
#             self._add_history("assistant", answer)
#             return answer
#
#         # ==================================================
#         # Ô∏è‚É£ FALLBACK: KH√îNG C√ì DATA ‚Üí D√ôNG LLM T·ªîNG QU√ÅT
#         # ==================================================
#         answer = self.general_llm_answer(question)
#         self._add_history("user", question)
#         self._add_history("assistant", answer)
#         return answer
# >>>>>>> Long
