import os
import re
import json
import logging
from google import genai
from google.genai import types
from typing import List, Dict

from src.search_engine import SearchEngine
from src.rag.prompt import SYSTEM_PROMPT, USER_PROMPT_TEMPLATE, LIBRARY_INFO
from config.rag_config import (
    GEMINI_API_KEY,
    GEMINI_MODEL,
    DEFAULT_TOP_K,
    SCORE_THRESHOLD,
    MIN_QUERY_LENGTH,
    TEMPERATURE,
    MAX_OUTPUT_TOKENS,
    QUERY_CACHE_THRESHOLD,
    SEARCH_EXPAND_FACTOR
)


# Logger cho module RAG
logger = logging.getLogger("RAGEngine")


class RAGEngine:
    """
    ========================================================
    ü§ñ RAGEngine
    --------------------------------------------------------
    Ch·ª©c nƒÉng:
    - Nh·∫≠n c√¢u h·ªèi ng∆∞·ªùi d√πng
    - Ph√¢n lo·∫°i: th·ªëng k√™ / n·ªôi quy / follow-up / t√¨m s√°ch / t·ªïng h·ª£p / fallback
    - Search vector DB
    - Build prompt cho Gemini
    - Cache l·∫°i c√¢u h·ªèi ƒë√£ h·ªèi (Query Memory)
    ========================================================
    """

    def __init__(self, top_k: int = DEFAULT_TOP_K):
        # ===============================
        # Ô∏è‚É£ SEARCH ENGINE (Vector DB + Embedder)
        # ===============================
        self.search_engine = SearchEngine()
        self.embedder = self.search_engine.embedder
        self.vector_db = self.search_engine.vector_db
        self.top_k = top_k

        # Initialize Gemini client
        self.client = genai.Client(api_key=GEMINI_API_KEY)
        self.last_docs = []  # For follow-up queries

    # ==================================================
    # FILTER GARBAGE QUERIES
    # ==================================================
    def is_garbage_query(self, query: str) -> bool:
        """
        L·ªçc c√°c c√¢u h·ªèi r√°c:
        - R·ªóng
        - Qu√° ng·∫Øn
        - To√†n s·ªë
        - Kh√¥ng ch·ª©a ch·ªØ c√°i
        """
        if not query or not query.strip():
            return True

        q = query.strip().lower()

        if len(q) < MIN_QUERY_LENGTH:
            return True
        if q.isdigit():
            return True

        # Kh√¥ng c√≥ ch·ªØ c√°i (k·ªÉ c·∫£ ti·∫øng Vi·ªát)
        if not re.search(r"[a-zA-Z√Ä-·ªπ]", q):
            return True

        return False

    # ==================================================
    # üìä NH·∫¨N DI·ªÜN C√ÇU H·ªéI TH·ªêNG K√ä
    # ==================================================
    def is_library_stats_query(self, question: str) -> bool:
        """
        V√≠ d·ª•:
        - "Th∆∞ vi·ªán c√≥ bao nhi√™u cu·ªën s√°ch?"
        - "T·ªïng s·ªë s√°ch l√† bao nhi√™u?"
        """
        q = question.lower()
        return any(k in q for k in [
            "bao nhi√™u s√°ch",
            "bao nhi√™u cu·ªën",
            "t·ªïng s·ªë s√°ch",
            "s·ªë l∆∞·ª£ng s√°ch",
            "th∆∞ vi·ªán c√≥ bao nhi√™u"
        ])

    # ==================================================
    # üèõÔ∏è NH·∫¨N DI·ªÜN C√ÇU H·ªéI N·ªòI QUY / GI·ªú GI·∫§C
    # ==================================================
    def is_library_info_query(self, question: str) -> bool:
        """
        V√≠ d·ª•:
        - M·∫•y gi·ªù m·ªü c·ª≠a?
        - Quy ƒë·ªãnh m∆∞·ª£n s√°ch?
        - Ph√≠ ph·∫°t th·∫ø n√†o?
        """
        q = question.lower()
        return any(k in q for k in [
            "m·ªü c·ª≠a",
            "ƒë√≥ng c·ª≠a",
            "gi·ªù m·ªü",
            "gi·ªù ƒë√≥ng",
            "gi·ªù l√†m vi·ªác",
            "n·ªôi quy",
            "quy ƒë·ªãnh",
            "m∆∞·ª£n s√°ch",
            "tr·∫£ s√°ch",
            "gia h·∫°n",
            "ph√≠ ph·∫°t"
        ])

    # ==================================================
    # üß† NH·∫¨N DI·ªÜN FOLLOW-UP QUESTION
    # ==================================================
    def is_followup_query(self, question: str) -> bool:
        """
        V√≠ d·ª•:
        - "Cu·ªën th·ª© th√¨ sao?"
        - "Cu·ªën n√†y ai vi·∫øt?"
        """
        if not self.last_docs:
            return False

        q = question.lower()
        return any(k in q for k in [
            "cu·ªën n√†y",
            "cu·ªën ƒë√≥",
            "cu·ªën th·ª©",
            "s√°ch n√†y",
            "s√°ch ƒë√≥"
        ])

    def answer_followup(self, question: str) -> str:
        """
        Tr·∫£ l·ªùi follow-up d·ª±a tr√™n danh s√°ch s√°ch l·∫ßn tr∆∞·ªõc
        """
        q = question.lower()
        match = re.search(r"th·ª©\s*(\d+)", q)

        if not match:
            return "‚ùå T√¥i ch∆∞a x√°c ƒë·ªãnh ƒë∆∞·ª£c cu·ªën s√°ch b·∫°n ƒëang h·ªèi."
        idx = int(match.group(1)) - 1

        if 0 <= idx < len(self.last_docs):
            b = self.last_docs[idx]
            return (
                f"üìò **{b['title']}**\n"
                f"- T√°c gi·∫£: {b['authors']}\n"
                f"- NƒÉm xu·∫•t b·∫£n: {b['published_year']}\n\n"
                f"{b.get('snippet','')}"
            )

        return "‚ùå Kh√¥ng t√¨m th·∫•y cu·ªën s√°ch b·∫°n y√™u c·∫ßu."

    # ==================================================
    # üß† C√ì C·∫¶N G·ªåI LLM ƒê·ªÇ T·ªîNG H·ª¢P KH√îNG?
    # ==================================================
    def needs_synthesis(self, question: str) -> bool:
        """
        N·∫øu ch·ªâ h·ªèi:
        - "S√°ch v·ªÅ AI" ‚Üí ch·ªâ list

        N·∫øu h·ªèi:
        - "N√™n ƒë·ªçc s√°ch n√†o?"
        - "So s√°nh gi√∫p t√¥i"
        ‚Üí c·∫ßn LLM t·ªïng h·ª£p
        """
        q = question.lower()
        return any(k in q for k in [
            "n√™n",
            "ph√π h·ª£p",
            "g·ª£i √Ω",
            "so s√°nh",
            "ƒë√°nh gi√°",
            "ph√¢n t√≠ch",
            "t·ªïng h·ª£p",
            "gi·∫£i th√≠ch",
            "v√¨ sao",
            "nh∆∞ th·∫ø n√†o"
        ])

    # ==================================================
    # üéØ L·ªåC THEO SCORE
    # ==================================================
    def apply_score_threshold(self, docs):
        """
        N·∫øu document t·ªët nh·∫•t < threshold ‚Üí coi nh∆∞ kh√¥ng c√≥ k·∫øt qu·∫£
        """
        if not docs:
            return []

        best = max(d.get("score", 0) for d in docs)
        return docs if best >= SCORE_THRESHOLD else []

    # ==================================================
    # üèõÔ∏è BUILD CONTEXT N·ªòI QUY TH∆Ø VI·ªÜN
    # ==================================================
    def _build_library_context(self) -> dict:
        """
        Convert LIBRARY_INFO th√†nh text cho prompt
        """
        return {
            "opening_hours": LIBRARY_INFO["opening_hours"],
            "library_rules": "\n".join(f"- {r}" for r in LIBRARY_INFO["library_rules"]),
            "borrow_policy": "\n".join(
                f"- {k}: {v}" for k, v in LIBRARY_INFO["borrow_policy"].items()
            ),
            "penalty_policy": "\n".join(
                f"- {k}: {v}" for k, v in LIBRARY_INFO["penalty_policy"].items()
            ),
        }

    # ==================================================
    # ü§ñ FALLBACK KHI KH√îNG C√ì DATA
    # ==================================================
    def gemini_fallback(self, question: str) -> str:
        """
        G·ªçi Gemini tr·∫£ l·ªùi chung chung nh∆∞ng:
        - Ph·∫£i n√≥i r√µ l√† th∆∞ vi·ªán kh√¥ng c√≥ d·ªØ li·ªáu
        - Kh√¥ng ƒë∆∞·ª£c b·ªãa s√°ch
        """
        prompt = f"""
B·∫°n l√† tr·ª£ l√Ω th∆∞ vi·ªán AI.

Th∆∞ vi·ªán KH√îNG c√≥ d·ªØ li·ªáu ph√π h·ª£p cho c√¢u h·ªèi:
"{question}"

Y√™u c·∫ßu:
- N√≥i r√µ kh√¥ng c√≥ d·ªØ li·ªáu
- KH√îNG b·ªãa t√™n s√°ch
"""
        try:
            response = self.client.models.generate_content(
                model=GEMINI_MODEL,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=TEMPERATURE,
                    max_output_tokens=MAX_OUTPUT_TOKENS
                )
            )
            return response.text.strip() if response and response.text else "‚ùå Xin l·ªói, t√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."
        except Exception as e:
            logger.error(f"Gemini fallback error: {e}")
            return "‚ùå Xin l·ªói, th∆∞ vi·ªán kh√¥ng c√≥ th√¥ng tin ph√π h·ª£p v·ªõi c√¢u h·ªèi c·ªßa b·∫°n."

    # ==================================================
    # GENERATE ANSWER
    # ==================================================
    def generate_answer(self, question: str) -> str:

        # ==================================================
        # Ô∏è‚É£ CH·∫∂N C√ÇU H·ªéI R√ÅC
        # ==================================================
        if self.is_garbage_query(question):
            return "‚ùå C√¢u h·ªèi kh√¥ng h·ª£p l·ªá ho·∫∑c qu√° ng·∫Øn."

        # ==================================================
        # Ô∏è‚É£ QUERY MEMORY (CACHE C√ÇU H·ªéI C≈®)
        # ==================================================
        q_vec = self.embedder.embed_text(question, is_query=True)

        if q_vec:
            cached = self.vector_db.search_query_memory(
                q_vec, threshold=QUERY_CACHE_THRESHOLD
            )
            if cached:
                logger.info("‚ö° Query memory HIT")
                return f"‚ö° {cached}"

        # ==================================================
        # Ô∏è‚É£ TH·ªêNG K√ä
        # ==================================================
        if self.is_library_stats_query(question):
            total = self.vector_db.get_collection_stats().get("count", 0)
            answer = f"üìö Hi·ªán t·∫°i th∆∞ vi·ªán c√≥ **{total} cu·ªën s√°ch** trong h·ªá th·ªëng."

            self.vector_db.add_query_memory(
                question, q_vec, answer, qtype="stats"
            )
            return answer

        # ==================================================
        # Ô∏è‚É£ N·ªòI QUY / GI·ªú GI·∫§C
        # ==================================================
        if self.is_library_info_query(question):
            ctx = self._build_library_context()

            prompt = f"""{SYSTEM_PROMPT}

{USER_PROMPT_TEMPLATE.format(
    question=question,
    books="(Kh√¥ng √°p d·ª•ng)",
    **ctx
)}
"""
            try:
                response = self.client.models.generate_content(
                    model=GEMINI_MODEL,
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        temperature=TEMPERATURE,
                        max_output_tokens=MAX_OUTPUT_TOKENS
                    )
                )
                answer = response.text.strip() if response and response.text else "‚ùå Kh√¥ng thÔøΩÔøΩÔøΩ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."
            except Exception as e:
                logger.error(f"Gemini API error: {e}")
                answer = "‚ùå Kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."

            self.vector_db.add_query_memory(
                question, q_vec, answer, qtype="library_info"
            )
            return answer

        # ==================================================
        # Ô∏è‚É£ FOLLOW-UP (KH√îNG CACHE)
        # ==================================================
        if self.is_followup_query(question):
            return self.answer_followup(question)

        # ==================================================
        # Ô∏è‚É£ BOOK RAG PIPELINE
        # ==================================================
        raw_docs = self.search_engine.search(
            query=question,
            top_k=self.top_k * SEARCH_EXPAND_FACTOR
        )

        # L·ªçc theo score
        docs = self.apply_score_threshold(raw_docs)

        if docs:
            # L∆∞u l·∫°i ƒë·ªÉ d√πng cho follow-up
            self.last_docs = docs[:self.top_k]

            # Build danh s√°ch s√°ch
            book_lines = [
                f"{i}. {d['title']} ‚Äì {d['authors']} ({d['published_year']})"
                for i, d in enumerate(self.last_docs, 1)
            ]

            books_text = "\n".join(book_lines)

            # ==================================================
            # .Ô∏è‚É£ CH·ªà LIST, KH√îNG T·ªîNG H·ª¢P
            # ==================================================
            if not self.needs_synthesis(question):
                answer = f"üìö Danh s√°ch s√°ch li√™n quan\n\n{books_text}"

                self.vector_db.add_query_memory(
                    question, q_vec, answer, qtype="rag_list"
                )
                return answer

            # ==================================================
            # .Ô∏è‚É£ C√ì G·ªåI LLM ƒê·ªÇ T·ªîNG H·ª¢P
            # ==================================================
            ctx = self._build_library_context()

            prompt = f"""{SYSTEM_PROMPT}

{USER_PROMPT_TEMPLATE.format(
    question=question,
    books=books_text,
    **ctx
)}
"""
            try:
                response = self.client.models.generate_content(
                    model=GEMINI_MODEL,
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        temperature=TEMPERATURE,
                        max_output_tokens=MAX_OUTPUT_TOKENS
                    )
                )
                synthesis = response.text.strip() if response and response.text else "‚ùå Kh√¥ng th·ªÉ t·ªïng h·ª£p th√¥ng tin."
            except Exception as e:
                logger.error(f"Gemini API error: {e}")
                synthesis = "‚ùå Kh√¥ng th·ªÉ t·ªïng h·ª£p th√¥ng tin."

            answer = f"""üìö Danh s√°ch s√°ch li√™n quan

{books_text}

üìù T·ªïng h·ª£p
{synthesis}
"""
            self.vector_db.add_query_memory(
                question, q_vec, answer, qtype="rag_synthesis"
            )
            return answer

        # ==================================================
        # Ô∏è‚É£ FALLBACK: KH√îNG C√ì DATA
        # ==================================================
        answer = self.gemini_fallback(question)

        self.vector_db.add_query_memory(
            question, q_vec, answer, qtype="fallback"
        )
        return answer
