import os
import re
import json
import logging
from google import genai
from google.genai import types
from typing import List, Dict

from src.search_engine import SearchEngine
from src.rag.prompt import (
    SYSTEM_PROMPT,
    USER_PROMPT_TEMPLATE,
    LIBRARY_INFO,
    FOLLOWUP_PROMPT_TEMPLATE,
    SMALLTALK_PROMPT_TEMPLATE,
    GENERAL_QA_PROMPT_TEMPLATE
)
from config.rag_config import (
    GEMINI_API_KEY,
    GEMINI_MODEL,
    DEFAULT_TOP_K,
    SCORE_THRESHOLD,
    MIN_QUERY_LENGTH,
    TEMPERATURE,
    MAX_OUTPUT_TOKENS,
    QUERY_CACHE_THRESHOLD,
    SEARCH_EXPAND_FACTOR
)


# Logger cho module RAG
logger = logging.getLogger("RAGEngine")


class RAGEngine:
    """
    ========================================================
    ü§ñ RAGEngine
    --------------------------------------------------------
    Ch·ª©c nƒÉng:
    - Nh·∫≠n c√¢u h·ªèi ng∆∞·ªùi d√πng
    - Ph√¢n lo·∫°i: th·ªëng k√™ / n·ªôi quy / follow-up / t√¨m s√°ch / t·ªïng h·ª£p / fallback
    - Search vector DB
    - Build prompt cho Gemini
    - Cache l·∫°i c√¢u h·ªèi ƒë√£ h·ªèi (Query Memory)
    ========================================================
    """

    def __init__(self, top_k: int = DEFAULT_TOP_K):
        # ===============================
        # Ô∏è‚É£ SEARCH ENGINE (Vector DB + Embedder)
        # ===============================
        self.search_engine = SearchEngine()
        self.embedder = self.search_engine.embedder
        self.vector_db = self.search_engine.vector_db
        self.top_k = top_k

        # Initialize Gemini client
        self.client = genai.Client(api_key=GEMINI_API_KEY)
        self.last_docs = []  # For follow-up queries
        self.last_books_text = ""
        self.history: list[dict] = []  # [{role: user/assistant, content: str}]

    def _add_history(self, role: str, content: str, max_turns: int = 6):
        # L∆∞u ng·∫Øn g·ªçn l·ªãch s·ª≠ ƒë·ªÉ LLM c√≥ ng·ªØ c·∫£nh follow-up
        self.history.append({"role": role, "content": content})
        if len(self.history) > max_turns * 2:
            self.history = self.history[-max_turns * 2 :]

    def _history_to_text(self) -> str:
        if not self.history:
            return "(ch∆∞a c√≥ l·ªãch s·ª≠)"
        lines = []
        for h in self.history[-8:]:
            prefix = "Ng∆∞·ªùi d√πng" if h["role"] == "user" else "Tr·ª£ l√Ω"
            lines.append(f"{prefix}: {h['content']}")
        return "\n".join(lines)

    def _shorten_text(self, text: str | None, max_len: int = 480) -> str:
        if not text:
            return "(kh√¥ng c√≥ m√¥ t·∫£)"
        text = text.strip()
        return text[:max_len] + "‚Ä¶" if len(text) > max_len else text

    def _books_to_context(self, docs: List[Dict]) -> str:
        lines = []
        for i, b in enumerate(docs, 1):
            snippet = self._shorten_text(b.get("richtext") or b.get("snippet"))
            lines.append(
                f"{i}. {b.get('title')} ‚Äì {b.get('authors')} ({b.get('publish_year')})\n{snippet}"
            )
        return "\n\n".join(lines)

    def is_smalltalk(self, question: str) -> bool:
        """
        Nh·∫≠n di·ªán c√¢u h·ªèi smalltalk / ch√†o h·ªèi.
        H·ªó tr·ª£ c·∫£ ti·∫øng Vi·ªát c√≥ d·∫•u v√† kh√¥ng d·∫•u.
        """
        q = question.lower().strip()

        # Lo·∫°i b·ªè d·∫•u c√¢u
        q = re.sub(r'[?.!,;:]', '', q)

        smalltalk_keywords = [
            # Ch√†o h·ªèi c√≥ d·∫•u
            "xin ch√†o", "ch√†o b·∫°n", "ch√†o", "ch√†o bu·ªïi s√°ng", "ch√†o bu·ªïi t·ªëi",
            # Ch√†o h·ªèi kh√¥ng d·∫•u
            "xin chao", "chao ban", "chao", "chao buoi sang", "chao buoi toi",
            # Ti·∫øng Anh
            "hello", "hi", "hey", "good morning", "good afternoon", "good evening",
            # C·∫£m ∆°n c√≥ d·∫•u
            "c·∫£m ∆°n", "c√°m ∆°n", "c·∫£m ∆°n b·∫°n", "c√°m ∆°n b·∫°n",
            # C·∫£m ∆°n kh√¥ng d·∫•u
            "cam on", "cam on ban",
            # Ti·∫øng Anh
            "thank", "thanks", "thank you", "tks", "ty",
            # T·∫°m bi·ªát c√≥ d·∫•u
            "t·∫°m bi·ªát", "h·∫πn g·∫∑p l·∫°i", "g·∫∑p l·∫°i sau",
            # T·∫°m bi·ªát kh√¥ng d·∫•u
            "tam biet", "hen gap lai", "gap lai sau",
            # Ti·∫øng Anh
            "bye", "goodbye", "see you", "see ya",
            # H·ªèi thƒÉm c√≥ d·∫•u
            "b·∫°n l√† ai", "t√™n g√¨", "kh·ªèe kh√¥ng", "b·∫°n ·ªïn kh√¥ng", "b·∫°n c√≥ kh·ªèe kh√¥ng",
            # H·ªèi thƒÉm kh√¥ng d·∫•u
            "ban la ai", "ten gi", "khoe khong", "ban on khong", "ban co khoe khong",
            # H·ªèi thƒÉm ti·∫øng Anh
            "how are you", "what's up", "who are you", "what is your name",
            # C√°c c√¢u ƒë∆°n gi·∫£n
            "alo", "yo", "hii", "hiii", "helloo", "helo"
        ]

        # Ki·ªÉm tra exact match tr∆∞·ªõc
        if q in smalltalk_keywords:
            return True

        # Ki·ªÉm tra contains
        return any(k in q for k in smalltalk_keywords)

    def smalltalk_answer(self, question: str) -> str:
        """
        D√πng Gemini tr·∫£ l·ªùi smalltalk m·ªôt c√°ch th√¥ng minh v√† t·ª± nhi√™n.
        """
        prompt = SMALLTALK_PROMPT_TEMPLATE.format(
            history=self._history_to_text(),
            question=question
        )

        try:
            response = self.client.models.generate_content(
                model=GEMINI_MODEL,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.7,  # Cao h∆°n ƒë·ªÉ tr·∫£ l·ªùi t·ª± nhi√™n h∆°n
                    max_output_tokens=150  # Ng·∫Øn g·ªçn
                )
            )
            return response.text.strip() if response and response.text else "üëã Ch√†o b·∫°n! M√¨nh c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?"
        except Exception as e:
            logger.error(f"Gemini smalltalk error: {e}")
            # Fallback n·∫øu API l·ªói
            return "üëã Ch√†o b·∫°n! M√¨nh l√† tr·ª£ l√Ω th∆∞ vi·ªán AI. B·∫°n c·∫ßn t√¨m s√°ch g√¨ h√¥m nay?"

    def general_llm_answer(self, question: str) -> str:
        """
        D√πng Gemini tr·∫£ l·ªùi c√°c c√¢u h·ªèi t·ªïng qu√°t khi kh√¥ng t√¨m th·∫•y s√°ch ph√π h·ª£p.
        """
        prompt = GENERAL_QA_PROMPT_TEMPLATE.format(
            history=self._history_to_text(),
            question=question
        )

        try:
            response = self.client.models.generate_content(
                model=GEMINI_MODEL,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.5,
                    max_output_tokens=MAX_OUTPUT_TOKENS
                )
            )
            return response.text.strip() if response and response.text else "‚ùå T√¥i ch∆∞a c√≥ c√¢u tr·∫£ l·ªùi ph√π h·ª£p."
        except Exception as e:
            logger.error(f"Gemini general QA error: {e}")
            return "‚ùå Kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y l√∫c n√†y. Vui l√≤ng th·ª≠ l·∫°i sau."

    def get_suggested_questions(self) -> List[str]:
        # Danh s√°ch g·ª£i √Ω m·∫∑c ƒë·ªãnh cho giao di·ªán chat
        return [
            "Th∆∞ vi·ªán m·ªü c·ª≠a l√∫c m·∫•y gi·ªù?",
            "L√†m sao ƒë·ªÉ gia h·∫°n s√°ch?",
            "C√≥ s√°ch n√†o v·ªÅ tr√≠ tu·ªá nh√¢n t·∫°o kh√¥ng?",
            "Ph√≠ ph·∫°t tr·∫£ s√°ch tr·ªÖ l√† bao nhi√™u?",
        ]

    def _is_book_related_query(self, question: str) -> bool:
        """
        Ki·ªÉm tra xem c√¢u h·ªèi c√≥ li√™n quan ƒë·∫øn vi·ªác t√¨m/h·ªèi v·ªÅ s√°ch kh√¥ng.
        D√πng ƒë·ªÉ quy·∫øt ƒë·ªãnh c√≥ n√™n d√πng cache s√°ch hay kh√¥ng.
        """
        q = question.lower()
        q = re.sub(r'[?.!,;:]', '', q)

        book_keywords = [
            # T·ª´ kh√≥a s√°ch ti·∫øng Vi·ªát
            "s√°ch", "cu·ªën", "quy·ªÉn", "t√†i li·ªáu", "gi√°o tr√¨nh", "truy·ªán",
            "ti·ªÉu thuy·∫øt", "t√°c ph·∫©m", "ebook", "pdf",
            # T·ª´ kh√≥a s√°ch kh√¥ng d·∫•u
            "sach", "cuon", "quyen", "tai lieu", "giao trinh", "truyen",
            "tieu thuyet", "tac pham",
            # T·ª´ kh√≥a t√¨m ki·∫øm
            "t√¨m", "t√¨m ki·∫øm", "g·ª£i √Ω", "ƒë·ªÅ xu·∫•t", "cho t√¥i", "c√≥ kh√¥ng",
            "tim", "tim kiem", "goi y", "de xuat", "cho toi", "co khong",
            # Th·ªÉ lo·∫°i s√°ch
            "python", "java", "programming", "l·∫≠p tr√¨nh", "lap trinh",
            "machine learning", "ai", "deep learning", "data science",
            "to√°n", "vƒÉn", "l·ªãch s·ª≠", "ƒë·ªãa l√Ω", "v·∫≠t l√Ω", "h√≥a h·ªçc",
            "toan", "van", "lich su", "dia ly", "vat ly", "hoa hoc",
            # Ti·∫øng Anh
            "book", "novel", "textbook", "recommend", "find", "search"
        ]

        return any(k in q for k in book_keywords)

    # ==================================================
    # FILTER GARBAGE QUERIES
    # ==================================================
    def is_garbage_query(self, query: str) -> bool:
        """
        L·ªçc c√°c c√¢u h·ªèi r√°c:
        - R·ªóng
        - Qu√° ng·∫Øn
        - To√†n s·ªë
        - Kh√¥ng ch·ª©a ch·ªØ c√°i
        """
        if not query or not query.strip():
            return True

        q = query.strip().lower()

        if len(q) < MIN_QUERY_LENGTH:
            return True
        if q.isdigit():
            return True

        # Kh√¥ng c√≥ ch·ªØ c√°i (k·ªÉ c·∫£ ti·∫øng Vi·ªát)
        if not re.search(r"[a-zA-Z√Ä-·ªπ]", q):
            return True

        return False

    # ==================================================
    # üìä NH·∫¨N DI·ªÜN C√ÇU H·ªéI TH·ªêNG K√ä
    # ==================================================
    def is_library_stats_query(self, question: str) -> bool:
        """
        V√≠ d·ª•:
        - "Th∆∞ vi·ªán c√≥ bao nhi√™u cu·ªën s√°ch?"
        - "T·ªïng s·ªë s√°ch l√† bao nhi√™u?"
        """
        q = question.lower()
        return any(k in q for k in [
            "bao nhi√™u s√°ch",
            "bao nhi√™u cu·ªën",
            "t·ªïng s·ªë s√°ch",
            "s·ªë l∆∞·ª£ng s√°ch",
            "th∆∞ vi·ªán c√≥ bao nhi√™u"
        ])

    # ==================================================
    # üèõÔ∏è NH·∫¨N DI·ªÜN C√ÇU H·ªéI N·ªòI QUY / GI·ªú GI·∫§C
    # ==================================================
    def is_library_info_query(self, question: str) -> bool:
        """
        V√≠ d·ª•:
        - M·∫•y gi·ªù m·ªü c·ª≠a?
        - Quy ƒë·ªãnh m∆∞·ª£n s√°ch?
        - Ph√≠ ph·∫°t th·∫ø n√†o?
        """
        q = question.lower()
        return any(k in q for k in [
            "m·ªü c·ª≠a",
            "ƒë√≥ng c·ª≠a",
            "gi·ªù m·ªü",
            "gi·ªù ƒë√≥ng",
            "gi·ªù l√†m vi·ªác",
            "n·ªôi quy",
            "quy ƒë·ªãnh",
            "m∆∞·ª£n s√°ch",
            "tr·∫£ s√°ch",
            "gia h·∫°n",
            "ph√≠ ph·∫°t"
        ])

    # ==================================================
    # üß† NH·∫¨N DI·ªÜN FOLLOW-UP QUESTION
    # ==================================================
    def is_followup_query(self, question: str) -> bool:
        """
        V√≠ d·ª•:
        - "Cu·ªën th·ª© th√¨ sao?"
        - "Cu·ªën n√†y ai vi·∫øt?"
        - "Trong s·ªë c√°c cu·ªën ƒë√≥, cu·ªën n√†o d·ªÖ h·ªçc nh·∫•t?"
        """
        if not self.last_docs:
            return False

        q = question.lower()
        keywords = [
            "cu·ªën n√†y",
            "cu·ªën ƒë√≥",
            "cu·ªën th·ª©",
            "s√°ch n√†y",
            "s√°ch ƒë√≥",
            "trong s·ªë",
            "cu·ªën n√†o",
            "c√°i n√†o",
            "d·ªÖ h·ªçc",
            "t·ªët nh·∫•t",
            "ph√π h·ª£p",
            "n√™n ch·ªçn",
            "·ªü tr√™n",
            "v·ª´a r·ªìi",
            "trong danh s√°ch",
        ]
        return any(k in q for k in keywords)

    def answer_followup(self, question: str) -> str:
        """
        Tr·∫£ l·ªùi follow-up d·ª±a tr√™n danh s√°ch s√°ch l·∫ßn tr∆∞·ªõc.
        - N·∫øu c√≥ ch·ªâ m·ª•c ("cu·ªën th·ª© 2") th√¨ tr·∫£ v·ªÅ s√°ch t∆∞∆°ng ·ª©ng.
        - N·∫øu l√† c√¢u ch·ªçn l·ªçc ("cu·ªën n√†o d·ªÖ h·ªçc nh·∫•t") th√¨ d√πng LLM t·ªïng h·ª£p tr√™n danh s√°ch hi·ªán c√≥.
        """
        if not self.last_docs:
            return "‚ùå T√¥i ch∆∞a c√≥ danh s√°ch s√°ch ƒë·ªÉ tham chi·∫øu."

        q = question.lower()
        match = re.search(r"th·ª©\s*(\d+)", q)
        if match:
            idx = int(match.group(1)) - 1
            if 0 <= idx < len(self.last_docs):
                b = self.last_docs[idx]
                snippet = self._shorten_text(b.get("richtext") or b.get("snippet"))
                return (
                    f"üìò **{b['title']}**\n"
                    f"- T√°c gi·∫£: {b['authors']}\n"
                    f"- NƒÉm xu·∫•t b·∫£n: {b['publish_year']}\n\n"
                    f"{snippet}"
                )
            return "‚ùå Kh√¥ng t√¨m th·∫•y cu·ªën s√°ch b·∫°n y√™u c·∫ßu."

        books_context = self._books_to_context(self.last_docs)
        prompt = FOLLOWUP_PROMPT_TEMPLATE.format(
            history=self._history_to_text(),
            previous_books=books_context,
            question=question
        )
        try:
            response = self.client.models.generate_content(
                model=GEMINI_MODEL,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=min(TEMPERATURE, 0.5),
                    max_output_tokens=MAX_OUTPUT_TOKENS
                )
            )
            return response.text.strip() if response and response.text else "‚ùå T√¥i ch∆∞a x√°c ƒë·ªãnh ƒë∆∞·ª£c c√¢u tr·∫£ l·ªùi."
        except Exception as e:
            logger.error(f"Gemini follow-up error: {e}")
            return "‚ùå Kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi follow-up."

    # ==================================================
    # üß† C√ì C·∫¶N G·ªåI LLM ƒê·ªÇ T·ªîNG H·ª¢P KH√îNG?
    # ==================================================
    def needs_synthesis(self, question: str) -> bool:
        """
        N·∫øu ch·ªâ h·ªèi:
        - "S√°ch v·ªÅ AI" ‚Üí ch·ªâ list

        N·∫øu h·ªèi:
        - "N√™n ƒë·ªçc s√°ch n√†o?"
        - "So s√°nh gi√∫p t√¥i"
        ‚Üí c·∫ßn LLM t·ªïng h·ª£p
        """
        q = question.lower()
        return any(k in q for k in [
            "n√™n",
            "ph√π h·ª£p",
            "g·ª£i √Ω",
            "so s√°nh",
            "ƒë√°nh gi√°",
            "ph√¢n t√≠ch",
            "t·ªïng h·ª£p",
            "gi·∫£i th√≠ch",
            "v√¨ sao",
            "nh∆∞ th·∫ø n√†o"
        ])

    # ==================================================
    # üéØ L·ªåC THEO SCORE
    # ==================================================
    def apply_score_threshold(self, docs):
        """
        N·∫øu document t·ªët nh·∫•t < threshold ‚Üí coi nh∆∞ kh√¥ng c√≥ k·∫øt qu·∫£
        """
        if not docs:
            return []

        best = max(d.get("score", 0) for d in docs)
        return docs if best >= SCORE_THRESHOLD else []

    # ==================================================
    # üèõÔ∏è BUILD CONTEXT N·ªòI QUY TH∆Ø VI·ªÜN
    # ==================================================
    def _build_library_context(self) -> dict:
        """
        Convert LIBRARY_INFO th√†nh text cho prompt
        """
        return {
            "opening_hours": LIBRARY_INFO["opening_hours"],
            "library_rules": "\n".join(f"- {r}" for r in LIBRARY_INFO["library_rules"]),
            "borrow_policy": "\n".join(
                f"- {k}: {v}" for k, v in LIBRARY_INFO["borrow_policy"].items()
            ),
            "penalty_policy": "\n".join(
                f"- {k}: {v}" for k, v in LIBRARY_INFO["penalty_policy"].items()
            ),
        }

    # ==================================================
    # ü§ñ FALLBACK KHI KH√îNG C√ì DATA
    # ==================================================
    def gemini_fallback(self, question: str) -> str:
        """
        G·ªçi Gemini tr·∫£ l·ªùi chung chung nh∆∞ng:
        - Ph·∫£i n√≥i r√µ l√† th∆∞ vi·ªán kh√¥ng c√≥ d·ªØ li·ªáu
        - Kh√¥ng ƒë∆∞·ª£c b·ªãa s√°ch
        """
        prompt = f"""
B·∫°n l√† tr·ª£ l√Ω th∆∞ vi·ªán AI.

Th∆∞ vi·ªán KH√îNG c√≥ d·ªØ li·ªáu ph√π h·ª£p cho c√¢u h·ªèi:
"{question}"

Y√™u c·∫ßu:
- N√≥i r√µ kh√¥ng c√≥ d·ªØ li·ªáu
- KH√îNG b·ªãa t√™n s√°ch
"""
        try:
            response = self.client.models.generate_content(
                model=GEMINI_MODEL,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=TEMPERATURE,
                    max_output_tokens=MAX_OUTPUT_TOKENS
                )
            )
            return response.text.strip() if response and response.text else "‚ùå Xin l·ªói, t√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."
        except Exception as e:
            logger.error(f"Gemini fallback error: {e}")
            return "‚ùå Xin l·ªói, th∆∞ vi·ªán kh√¥ng c√≥ th√¥ng tin ph√π h·ª£p v·ªõi c√¢u h·ªèi c·ªßa b·∫°n."

    # ==================================================
    # GENERATE ANSWER
    # ==================================================
    def generate_answer(self, question: str) -> str:

        # ==================================================
        # Ô∏è‚É£ CH·∫∂N C√ÇU H·ªéI R√ÅC
        # ==================================================
        if self.is_garbage_query(question):
            return "‚ùå C√¢u h·ªèi kh√¥ng h·ª£p l·ªá ho·∫∑c qu√° ng·∫Øn."

        # ==================================================
        # Ô∏è‚É£ SMALLTALK / CH√ÄO H·ªéI (b·ªè qua cache ƒë·ªÉ tr√°nh hit s√°ch c≈©)
        # ==================================================
        if self.is_smalltalk(question):
            answer = self.smalltalk_answer(question)
            self._add_history("user", question)
            self._add_history("assistant", answer)
            return answer

        # ==================================================
        # Ô∏è‚É£ QUERY MEMORY (CACHE C√ÇU H·ªéI C≈®)
        # ==================================================
        q_vec = self.embedder.embed_text(question, is_query=True)
        if q_vec:
            cached = self.vector_db.search_query_memory(
                q_vec, threshold=QUERY_CACHE_THRESHOLD
            )
            # Skip cache n·∫øu cache answer l√† danh s√°ch s√°ch nh∆∞ng query kh√¥ng li√™n quan s√°ch
            if cached:
                is_book_cache = "üìö Danh s√°ch s√°ch" in cached or "Danh s√°ch s√°ch li√™n quan" in cached
                if is_book_cache and not self._is_book_related_query(question):
                    logger.info("‚ö†Ô∏è Query memory SKIP (cached books for non-book query)")
                else:
                    logger.info("‚ö° Query memory HIT")
                    answer = f"‚ö° {cached}"
                    self._add_history("user", question)
                    self._add_history("assistant", answer)
                    return answer

        # ==================================================
        # Ô∏è‚É£ TH·ªêNG K√ä
        # ==================================================
        if self.is_library_stats_query(question):
            total = self.vector_db.get_collection_stats().get("count", 0)
            answer = f"üìö Hi·ªán t·∫°i th∆∞ vi·ªán c√≥ **{total} cu·ªën s√°ch** trong h·ªá th·ªëng."

            self.vector_db.add_query_memory(
                question, q_vec, answer, qtype="stats"
            )
            self._add_history("user", question)
            self._add_history("assistant", answer)
            return answer

        # ==================================================
        # Ô∏è‚É£ N·ªòI QUY / GI·ªú GI·∫§C
        # ==================================================
        if self.is_library_info_query(question):
            ctx = self._build_library_context()
            prompt = f"""{SYSTEM_PROMPT}

L·ªãch s·ª≠ h·ªôi tho·∫°i g·∫ßn ƒë√¢y:
{self._history_to_text()}

{USER_PROMPT_TEMPLATE.format(
    question=question,
    books="(Kh√¥ng √°p d·ª•ng)",
    **ctx
)}
"""
            try:
                response = self.client.models.generate_content(
                    model=GEMINI_MODEL,
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        temperature=TEMPERATURE,
                        max_output_tokens=MAX_OUTPUT_TOKENS
                    )
                )
                answer = response.text.strip() if response and response.text else "‚ùå Kh√¥ng thÔøΩÔøΩÔøΩ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."
            except Exception as e:
                logger.error(f"Gemini API error: {e}")
                answer = "‚ùå Kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."

            self.vector_db.add_query_memory(
                question, q_vec, answer, qtype="library_info"
            )
            self._add_history("user", question)
            self._add_history("assistant", answer)
            return answer

        # ==================================================
        # Ô∏è‚É£ FOLLOW-UP (KH√îNG CACHE)
        # ==================================================
        if self.is_followup_query(question):
            answer = self.answer_followup(question)
            self._add_history("user", question)
            self._add_history("assistant", answer)
            return answer

        # ==================================================
        # Ô∏è‚É£ BOOK RAG PIPELINE
        # ==================================================
        raw_docs = self.search_engine.search(
            query=question,
            top_k=self.top_k * SEARCH_EXPAND_FACTOR
        )

        # L·ªçc theo score
        docs = self.apply_score_threshold(raw_docs)

        if docs:
            # L∆∞u l·∫°i ƒë·ªÉ d√πng cho follow-up
            self.last_docs = docs[:self.top_k]
            self.last_books_text = "\n".join(
                f"{i}. {d['title']} ‚Äì {d['authors']} ({d['publish_year']})"
                for i, d in enumerate(self.last_docs, 1)
            )

            # Build danh s√°ch s√°ch
            book_lines = [
                f"{i}. {d['title']} ‚Äì {d['authors']} ({d['publish_year']})"
                for i, d in enumerate(self.last_docs, 1)
            ]

            books_text = "\n".join(book_lines)

            # ==================================================
            # .Ô∏è‚É£ CH·ªà LIST, KH√îNG T·ªîNG H·ª¢P
            # ==================================================
            if not self.needs_synthesis(question):
                answer = f"üìö Danh s√°ch s√°ch li√™n quan\n\n{books_text}"

                self.vector_db.add_query_memory(
                    question, q_vec, answer, qtype="rag_list"
                )
                self._add_history("user", question)
                self._add_history("assistant", answer)
                return answer

            # ==================================================
            # .Ô∏è‚É£ C√ì G·ªåI LLM ƒê·ªÇ T·ªîNG H·ª¢P
            # ==================================================
            ctx = self._build_library_context()

            prompt = f"""{SYSTEM_PROMPT}

L·ªãch s·ª≠ h·ªôi tho·∫°i g·∫ßn ƒë√¢y:
{self._history_to_text()}

{USER_PROMPT_TEMPLATE.format(
    question=question,
    books=books_text,
    **ctx
)}
"""
            try:
                response = self.client.models.generate_content(
                    model=GEMINI_MODEL,
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        temperature=TEMPERATURE,
                        max_output_tokens=MAX_OUTPUT_TOKENS
                    )
                )
                synthesis = response.text.strip() if response and response.text else "‚ùå Kh√¥ng th·ªÉ t·ªïng h·ª£p th√¥ng tin."
            except Exception as e:
                logger.error(f"Gemini API error: {e}")
                synthesis = "‚ùå Kh√¥ng th·ªÉ t·ªïng h·ª£p th√¥ng tin."

            answer = f"""üìö Danh s√°ch s√°ch li√™n quan

{books_text}

üìù T·ªïng h·ª£p
{synthesis}
"""
            self.vector_db.add_query_memory(
                question, q_vec, answer, qtype="rag_synthesis"
            )
            self._add_history("user", question)
            self._add_history("assistant", answer)
            return answer

        # ==================================================
        # Ô∏è‚É£ FALLBACK: KH√îNG C√ì DATA ‚Üí D√ôNG LLM T·ªîNG QU√ÅT
        # ==================================================
        answer = self.general_llm_answer(question)
        self._add_history("user", question)
        self._add_history("assistant", answer)
        return answer
