"""
=====================================================
RAG ENGINE - MERGED VERSION
=====================================================
Gi·ªØ l·∫°i: origin/main (Session persistent, ModelManager)
Th√™m t·ª´ HEAD:
1. Smalltalk detection chi ti·∫øt (ti·∫øng Vi·ªát c√≥/kh√¥ng d·∫•u)
2. _is_book_related_query() ƒë·ªÉ skip cache th√¥ng minh
3. Prompt templates cho smalltalk v√† general QA
=====================================================
"""

import os
import re
import json
import logging
from typing import List, Dict

from config.settings import settings
from src.search_engine import SearchEngine
from src.rag.prompt import SYSTEM_PROMPT, USER_PROMPT_TEMPLATE, LIBRARY_INFO
from src.rag.model_manager import ModelManager
from config.rag_config import (
    GEMINI_API_KEYS,
    GEMINI_MODELS,
    GEMINI_MODEL,
    DEFAULT_TOP_K,
    SCORE_THRESHOLD,
    MIN_QUERY_LENGTH,
    TEMPERATURE,
    MAX_OUTPUT_TOKENS,
    QUERY_CACHE_THRESHOLD,
    SEARCH_EXPAND_FACTOR
)

# Logger cho module RAG
logger = logging.getLogger("RAGEngine")


# =====================================================
# üí¨ PROMPT TEMPLATES (TH√äM T·ª™ HEAD)
# =====================================================
# L√Ω do: C√°c template n√†y gi√∫p Gemini tr·∫£ l·ªùi th√¥ng minh h∆°n
# cho c√°c tr∆∞·ªùng h·ª£p smalltalk v√† c√¢u h·ªèi t·ªïng qu√°t

SMALLTALK_PROMPT_TEMPLATE = """
B·∫°n l√† tr·ª£ l√Ω AI th√¢n thi·ªán c·ªßa th∆∞ vi·ªán.

L·ªãch s·ª≠ h·ªôi tho·∫°i:
{history}

Ng∆∞·ªùi d√πng n√≥i: "{question}"

H√£y tr·∫£ l·ªùi m·ªôt c√°ch th√¢n thi·ªán, t·ª± nhi√™n b·∫±ng ti·∫øng Vi·ªát.
- N·∫øu l√† l·ªùi ch√†o: ch√†o l·∫°i v√† gi·ªõi thi·ªáu ng·∫Øn g·ªçn b·∫°n c√≥ th·ªÉ gi√∫p t√¨m s√°ch, tra c·ª©u th√¥ng tin th∆∞ vi·ªán
- N·∫øu l√† c·∫£m ∆°n: ƒë√°p l·∫°i l·ªãch s·ª± v√† h·ªèi c√≥ c·∫ßn gi√∫p g√¨ th√™m kh√¥ng
- N·∫øu l√† t·∫°m bi·ªát: ch√†o t·∫°m bi·ªát th√¢n thi·ªán
- N·∫øu h·ªèi v·ªÅ b·∫°n: gi·ªõi thi·ªáu b·∫°n l√† tr·ª£ l√Ω AI th∆∞ vi·ªán
- N·∫øu l√† c√¢u h·ªèi chung: tr·∫£ l·ªùi ng·∫Øn g·ªçn, th√¥ng minh

Tr·∫£ l·ªùi ng·∫Øn g·ªçn (1-3 c√¢u), th√¢n thi·ªán, c√≥ th·ªÉ d√πng emoji ph√π h·ª£p.
KH√îNG ƒë∆∞a ra danh s√°ch s√°ch n·∫øu kh√¥ng ƒë∆∞·ª£c h·ªèi.
"""

GENERAL_QA_PROMPT_TEMPLATE = """
B·∫°n l√† tr·ª£ l√Ω AI th√¥ng minh c·ªßa th∆∞ vi·ªán.

L·ªãch s·ª≠ h·ªôi tho·∫°i g·∫ßn ƒë√¢y:
{history}

C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: "{question}"

H∆∞·ªõng d·∫´n tr·∫£ l·ªùi:
1. N·∫øu l√† c√¢u h·ªèi ki·∫øn th·ª©c chung (to√°n, khoa h·ªçc, l·ªãch s·ª≠, v.v.): Tr·∫£ l·ªùi ch√≠nh x√°c, ng·∫Øn g·ªçn
2. N·∫øu l√† c√¢u h·ªèi v·ªÅ s√°ch nh∆∞ng th∆∞ vi·ªán kh√¥ng c√≥: N√≥i r√µ th∆∞ vi·ªán ch∆∞a c√≥ s√°ch ph√π h·ª£p
3. N·∫øu l√† c√¢u h·ªèi c√° nh√¢n ho·∫∑c kh√¥ng ph√π h·ª£p: Nh·∫π nh√†ng t·ª´ ch·ªëi v√† h∆∞·ªõng v·ªÅ ch·ª©c nƒÉng th∆∞ vi·ªán
4. N·∫øu l√† c√¢u h·ªèi ti·∫øp n·ªëi: D·ª±a v√†o l·ªãch s·ª≠ ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c

Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, th√¢n thi·ªán, ch√≠nh x√°c. C√≥ th·ªÉ d√πng emoji ph√π h·ª£p.
KH√îNG b·ªãa t√™n s√°ch ho·∫∑c th√¥ng tin kh√¥ng ch√≠nh x√°c.
"""

FOLLOWUP_PROMPT_TEMPLATE = """
B·∫°n l√† TR·ª¢ L√ù TH∆Ø VI·ªÜN AI th√¥ng minh.

============================
L·ªãch s·ª≠ h·ªôi tho·∫°i:
============================
{history}

============================
Danh s√°ch s√°ch ƒë√£ ƒë·ªÅ c·∫≠p tr∆∞·ªõc ƒë√≥:
============================
{previous_books}

============================
C√¢u h·ªèi ti·∫øp theo c·ªßa ng∆∞·ªùi d√πng:
============================
{question}

============================
H∆∞·ªõng d·∫´n tr·∫£ l·ªùi:
============================
1. ƒê√¢y l√† c√¢u h·ªèi TI·∫æP N·ªêI, h√£y d·ª±a v√†o ng·ªØ c·∫£nh tr∆∞·ªõc ƒë√≥
2. N·∫øu h·ªèi "cu·ªën n√†o hay/d·ªÖ/t·ªët nh·∫•t" ‚Üí ch·ªçn t·ª´ danh s√°ch s√°ch ƒë√£ ƒë·ªÅ c·∫≠p v√† gi·∫£i th√≠ch l√Ω do
3. N·∫øu h·ªèi "cu·ªën th·ª© X" ‚Üí tham chi·∫øu ƒë·∫øn v·ªã tr√≠ trong danh s√°ch
4. N·∫øu h·ªèi chi ti·∫øt v·ªÅ m·ªôt cu·ªën ‚Üí cung c·∫•p th√¥ng tin c√≥ s·∫µn
5. Tr·∫£ l·ªùi t·ª± nhi√™n, th√¢n thi·ªán, c√≥ th·ªÉ d√πng emoji
6. KH√îNG b·ªãa th√¥ng tin kh√¥ng c√≥
"""


class ChatSession:
    """
    L∆∞u tr·ªØ tr·∫°ng th√°i h·ªôi tho·∫°i c·ªßa m·ªôt user/session.
    Persists to disk to survive restarts.
    """
    def __init__(self, session_id: str):
        self.session_id = session_id
        self.history: List[Dict] = []
        self.last_search_results: List[Dict] = []
        self.file_path = os.path.join(
            settings.DATA_PROCESSED_DIR,
            "chat_sessions",
            f"rag_{session_id}.json"
        )

    def add_message(self, role: str, text: str):
        self.history.append({"role": role, "text": text})
        self.save()

    def get_history_text(self, max_turns: int = 8) -> str:
        """Chuy·ªÉn history th√†nh text cho prompt (TH√äM T·ª™ HEAD)"""
        if not self.history:
            return "(ch∆∞a c√≥ l·ªãch s·ª≠)"
        lines = []
        for h in self.history[-max_turns:]:
            prefix = "Ng∆∞·ªùi d√πng" if h["role"] == "user" else "Tr·ª£ l√Ω"
            lines.append(f"{prefix}: {h['text']}")
        return "\n".join(lines)

    def save(self):
        try:
            os.makedirs(os.path.dirname(self.file_path), exist_ok=True)
            data = {
                "session_id": self.session_id,
                "history": self.history,
                "last_search_results": self.last_search_results
            }
            with open(self.file_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"Failed to save session {self.session_id}: {e}")

    def load(self):
        try:
            if os.path.exists(self.file_path):
                with open(self.file_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    self.history = data.get("history", [])
                    self.last_search_results = data.get("last_search_results", [])
        except Exception as e:
            logger.error(f"Failed to load session {self.session_id}: {e}")


class RAGEngine:
    """
    ========================================================
    ü§ñ RAGEngine (Merged Version)
    --------------------------------------------------------
    Ch·ª©c nƒÉng:
    - Qu·∫£n l√Ω session chat (Persistent)
    - Ph√¢n lo·∫°i intent th√¥ng minh (TH√äM: Smalltalk chi ti·∫øt)
    - RAG flow v·ªõi context memory
    - Multi-Model Rotation & Rate Limit Handling
    ========================================================
    """

    def __init__(self, top_k: int = DEFAULT_TOP_K):
        # 1. SEARCH ENGINE
        self.search_engine = SearchEngine()
        self.embedder = self.search_engine.embedder
        self.vector_db = self.search_engine.vector_db
        self.top_k = top_k

        # 2. Model Manager (multi-key rotation)
        self.model_manager = ModelManager(
            api_keys=GEMINI_API_KEYS,
            models=GEMINI_MODELS
        )

        # 3. Session storage {session_id: ChatSession}
        self.sessions: Dict[str, ChatSession] = {}

    def get_session(self, session_id: str) -> ChatSession:
        if session_id not in self.sessions:
            session = ChatSession(session_id)
            session.load()
            self.sessions[session_id] = session
        return self.sessions[session_id]

    # ==================================================
    # üÜï SMALLTALK DETECTION (TH√äM T·ª™ HEAD)
    # ==================================================
    # L√Ω do th√™m: Phi√™n b·∫£n HEAD c√≥ nh·∫≠n di·ªán smalltalk chi ti·∫øt h∆°n,
    # h·ªó tr·ª£ c·∫£ ti·∫øng Vi·ªát c√≥ d·∫•u v√† kh√¥ng d·∫•u, gi√∫p bot ph·∫£n h·ªìi
    # ch√≠nh x√°c h∆°n khi user ch√†o h·ªèi

    def is_smalltalk(self, question: str) -> bool:
        """
        Nh·∫≠n di·ªán c√¢u h·ªèi smalltalk / ch√†o h·ªèi.
        H·ªó tr·ª£ c·∫£ ti·∫øng Vi·ªát c√≥ d·∫•u v√† kh√¥ng d·∫•u.
        """
        q = question.lower().strip()
        q = re.sub(r'[?.!,;:]', '', q)

        smalltalk_keywords = [
            # Ch√†o h·ªèi c√≥ d·∫•u
            "xin ch√†o", "ch√†o b·∫°n", "ch√†o", "ch√†o bu·ªïi s√°ng", "ch√†o bu·ªïi t·ªëi",
            # Ch√†o h·ªèi kh√¥ng d·∫•u
            "xin chao", "chao ban", "chao", "chao buoi sang", "chao buoi toi",
            # Ti·∫øng Anh
            "hello", "hi", "hey", "good morning", "good afternoon", "good evening",
            # C·∫£m ∆°n c√≥ d·∫•u
            "c·∫£m ∆°n", "c√°m ∆°n", "c·∫£m ∆°n b·∫°n", "c√°m ∆°n b·∫°n",
            # C·∫£m ∆°n kh√¥ng d·∫•u
            "cam on", "cam on ban",
            # Ti·∫øng Anh
            "thank", "thanks", "thank you", "tks", "ty",
            # T·∫°m bi·ªát c√≥ d·∫•u
            "t·∫°m bi·ªát", "h·∫πn g·∫∑p l·∫°i", "g·∫∑p l·∫°i sau",
            # T·∫°m bi·ªát kh√¥ng d·∫•u
            "tam biet", "hen gap lai", "gap lai sau",
            # Ti·∫øng Anh
            "bye", "goodbye", "see you", "see ya",
            # H·ªèi thƒÉm c√≥ d·∫•u
            "b·∫°n l√† ai", "t√™n g√¨", "kh·ªèe kh√¥ng", "b·∫°n ·ªïn kh√¥ng", "b·∫°n c√≥ kh·ªèe kh√¥ng",
            # H·ªèi thƒÉm kh√¥ng d·∫•u
            "ban la ai", "ten gi", "khoe khong", "ban on khong", "ban co khoe khong",
            # H·ªèi thƒÉm ti·∫øng Anh
            "how are you", "what's up", "who are you", "what is your name",
            # C√°c c√¢u ƒë∆°n gi·∫£n
            "alo", "yo", "hii", "hiii", "helloo", "helo"
        ]

        if q in smalltalk_keywords:
            return True
        return any(k in q for k in smalltalk_keywords)

    def answer_smalltalk(self, question: str, session: ChatSession) -> str:
        """
        D√πng Gemini tr·∫£ l·ªùi smalltalk th√¥ng minh (TH√äM T·ª™ HEAD)
        L√Ω do: Thay v√¨ tr·∫£ l·ªùi c·ª©ng, d√πng LLM ƒë·ªÉ tr·∫£ l·ªùi t·ª± nhi√™n h∆°n
        """
        prompt = SMALLTALK_PROMPT_TEMPLATE.format(
            history=session.get_history_text(),
            question=question
        )
        return self._call_gemini(prompt, temperature=0.7, max_tokens=150)

    # ==================================================
    # üÜï BOOK RELATED CHECK (TH√äM T·ª™ HEAD)
    # ==================================================
    # L√Ω do th√™m: Gi√∫p skip cache s√°ch khi user h·ªèi c√¢u kh√¥ng li√™n quan s√°ch
    # V√≠ d·ª•: "xin ch√†o" kh√¥ng n√™n hit cache c√≥ danh s√°ch s√°ch

    def _is_book_related_query(self, question: str) -> bool:
        """
        Ki·ªÉm tra xem c√¢u h·ªèi c√≥ li√™n quan ƒë·∫øn vi·ªác t√¨m/h·ªèi v·ªÅ s√°ch kh√¥ng.
        D√πng ƒë·ªÉ quy·∫øt ƒë·ªãnh c√≥ n√™n d√πng cache s√°ch hay kh√¥ng.
        """
        q = question.lower()
        q = re.sub(r'[?.!,;:]', '', q)

        book_keywords = [
            # T·ª´ kh√≥a s√°ch ti·∫øng Vi·ªát
            "s√°ch", "cu·ªën", "quy·ªÉn", "t√†i li·ªáu", "gi√°o tr√¨nh", "truy·ªán",
            "ti·ªÉu thuy·∫øt", "t√°c ph·∫©m", "ebook", "pdf",
            # T·ª´ kh√≥a s√°ch kh√¥ng d·∫•u
            "sach", "cuon", "quyen", "tai lieu", "giao trinh", "truyen",
            "tieu thuyet", "tac pham",
            # T·ª´ kh√≥a t√¨m ki·∫øm
            "t√¨m", "t√¨m ki·∫øm", "g·ª£i √Ω", "ƒë·ªÅ xu·∫•t", "cho t√¥i", "c√≥ kh√¥ng",
            "tim", "tim kiem", "goi y", "de xuat", "cho toi", "co khong",
            # Th·ªÉ lo·∫°i s√°ch
            "python", "java", "programming", "l·∫≠p tr√¨nh", "lap trinh",
            "machine learning", "ai", "deep learning", "data science",
            "to√°n", "vƒÉn", "l·ªãch s·ª≠", "ƒë·ªãa l√Ω", "v·∫≠t l√Ω", "h√≥a h·ªçc",
            "toan", "van", "lich su", "dia ly", "vat ly", "hoa hoc",
            # Ti·∫øng Anh
            "book", "novel", "textbook", "recommend", "find", "search"
        ]

        return any(k in q for k in book_keywords)

    # ==================================================
    # INTENT CLASSIFICATION (C·∫¢I TI·∫æN)
    # ==================================================
    def classify_intent(self, query: str, session: ChatSession) -> str:
        q = query.strip().lower()

        # 1. Garbage check
        if len(q) < 2 or not re.search(r"[a-zA-Z\u00c0-\u1ef90-9]", q):
            return "GARBAGE"

        # 1b. Library stats check (NEW): ∆∞u ti√™n cao, tr√°nh b·ªã search s√°ch
        if self.is_library_stats_query(query):
            return "STATS"

        # 2. Smalltalk check
        if self.is_smalltalk(query):
            return "SMALLTALK"

        # 3. Follow-up check
        if session.last_search_results:
            followup_keywords = [
                "cu·ªën n√†y", "cu·ªën ƒë√≥", "cu·ªën th·ª©", "s√°ch n√†y", "s√°ch ƒë√≥",
                "chi ti·∫øt", "n√≥ n√≥i v·ªÅ", "t√°c gi·∫£ l√† ai", "gi√° bao nhi√™u",
                "trong s·ªë", "cu·ªën n√†o", "c√°i n√†o", "d·ªÖ h·ªçc", "t·ªët nh·∫•t",
                "ph√π h·ª£p", "n√™n ch·ªçn", "·ªü tr√™n", "v·ª´a r·ªìi", "trong danh s√°ch"
            ]
            if any(k in q for k in followup_keywords):
                return "FOLLOWUP"
            if re.search(r"(cu·ªën|s·ªë|quy·ªÉn)\s*\d+", q):
                return "FOLLOWUP"

        # 4. Default
        return "SEARCH"

    def is_library_stats_query(self, q: str) -> bool:
        ql = q.lower()
        # B·ªï sung th√™m m·∫´u h·ªèi ph·ªï bi·∫øn
        return any(k in ql for k in [
            "bao nhi√™u cu·ªën s√°ch",
            "bao nhi√™u quy·ªÉn s√°ch",
            "bao nhi√™u s√°ch",
            "t·ªïng s·ªë s√°ch",
            "s·ªë l∆∞·ª£ng s√°ch",
            "th∆∞ vi·ªán c√≥ bao nhi√™u",
            "hi·ªán c√≥ bao nhi√™u",
        ])

    def _normalize_book_query(self, question: str) -> str:
        """Chu·∫©n ho√° m·ªôt s·ªë c√¢u g·ª£i √Ω ƒë·ªÉ search tr√∫ng ch·ªß ƒë·ªÅ h∆°n."""
        q = question.strip()
        ql = q.lower()

        # N·∫øu user h·ªèi ki·ªÉu: "S√°ch Machine Learning hay nh·∫•t" ‚Üí th√™m t·ª´ kho√° 's√°ch'
        if "machine learning" in ql and "s√°ch" in ql:
            return "s√°ch machine learning"

        # "T√¨m s√°ch v·ªÅ Python" hay "s√°ch python" ‚Üí chu·∫©n ho√°
        if "python" in ql and ("t√¨m" in ql or "s√°ch" in ql):
            return "s√°ch python"

        return q

    # ==================================================
    # HANDLERS
    # ==================================================
    def answer_greeting(self) -> str:
        return "üëã Xin ch√†o! T√¥i l√† tr·ª£ l√Ω th∆∞ vi·ªán AI. T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n h√¥m nay? (T√¨m s√°ch, h·ªèi n·ªôi quy, v.v...)"

    def answer_followup(self, question: str, session: ChatSession) -> str:
        """Tr·∫£ l·ªùi follow-up d·ª±a tr√™n last_search_results c·ªßa session"""
        q = question.lower()

        # 1. Check for "all" / "summarize all"
        if any(k in q for k in ["t·∫•t c·∫£", "c·∫£ hai", "c·∫£ 2", "c·∫£ 3", "m·ªçi cu·ªën", "nh·ªØng cu·ªën n√†y", "c√°c cu·ªën"]):
            books_text = "\n".join([
                f"{i}. {d['title']} ‚Äì {d['authors']}"
                for i, d in enumerate(session.last_search_results, 1)
            ])
            # TH√äM: D√πng FOLLOWUP_PROMPT_TEMPLATE thay v√¨ prompt c·ª©ng
            prompt = FOLLOWUP_PROMPT_TEMPLATE.format(
                history=session.get_history_text(),
                previous_books=books_text,
                question=question
            )
            return self._call_gemini(prompt)

        # 2. Extract specific index
        idx = -1
        text_nums = {
            "m·ªôt": 1, "hai": 2, "ba": 3, "b·ªën": 4, "nƒÉm": 5,
            "nh·∫•t": 1, "nh√¨": 2, "ƒë·∫ßu ti√™n": 1,
            "cu·ªëi c√πng": len(session.last_search_results)
        }

        text_pattern = r"(th·ª©|s·ªë|cu·ªën|quy·ªÉn)\s*(" + "|".join(text_nums.keys()) + r")"
        match_text = re.search(text_pattern, q)
        digit_pattern = r"(?:th·ª©|s·ªë|cu·ªën|quy·ªÉn|^)\s*(\d+)"
        match_digit = re.search(digit_pattern, q)

        if match_text:
            key = match_text.group(2)
            idx = text_nums.get(key, 0) - 1
        elif match_digit:
            idx = int(match_digit.group(1)) - 1

        # 3. Return info if index valid
        if 0 <= idx < len(session.last_search_results):
            b = session.last_search_results[idx]
            return (
                f"üìò **{b['title']}**\n"
                f"- T√°c gi·∫£: {b['authors']}\n"
                f"- NƒÉm xu·∫•t b·∫£n: {b['publish_year']}\n"
                f"- M√£ s√°ch: {b['identifier']}\n\n"
                f"üìù **N·ªôi dung:**\n{b.get('richtext','')[:1000]}..."
            )

        # 4. TH√äM: D√πng LLM ƒë·ªÉ tr·∫£ l·ªùi follow-up ph·ª©c t·∫°p (t·ª´ HEAD)
        if session.last_search_results:
            books_text = "\n".join([
                f"{i}. {d['title']} ‚Äì {d['authors']} ({d['publish_year']})"
                for i, d in enumerate(session.last_search_results, 1)
            ])
            prompt = FOLLOWUP_PROMPT_TEMPLATE.format(
                history=session.get_history_text(),
                previous_books=books_text,
                question=question
            )
            return self._call_gemini(prompt)

        return "B·∫°n mu·ªën h·ªèi v·ªÅ cu·ªën s√°ch s·ªë m·∫•y? (V√≠ d·ª•: 'cu·ªën s·ªë 1', 'quy·ªÉn ƒë·∫ßu ti√™n')"

    def needs_synthesis(self, question: str) -> bool:
        q = question.lower()
        return any(k in q for k in [
            "n√™n", "ph√π h·ª£p", "g·ª£i √Ω", "so s√°nh", "ƒë√°nh gi√°",
            "ph√¢n t√≠ch", "t·ªïng h·ª£p", "gi·∫£i th√≠ch", "v√¨ sao", "nh∆∞ th·∫ø n√†o"
        ])

    def _build_library_context(self) -> dict:
        return {
            "opening_hours": LIBRARY_INFO["opening_hours"],
            "library_rules": "\n".join(f"- {r}" for r in LIBRARY_INFO["library_rules"]),
            "borrow_policy": "\n".join(f"- {k}: {v}" for k, v in LIBRARY_INFO["borrow_policy"].items()),
            "penalty_policy": "\n".join(f"- {k}: {v}" for k, v in LIBRARY_INFO["penalty_policy"].items()),
        }

    # ==================================================
    # MAIN GENERATE FUNCTION (C·∫¢I TI·∫æN)
    # ==================================================
    def generate_answer(self, question: str, session_id: str = "default", filters: dict = None) -> str:
        """
        Generate answer for a chat question.

        Args:
            question: User's question
            session_id: Session identifier for conversation context
            filters: Optional filters from FE (category, authors, year, etc.)

        Returns:
            AI-generated answer string
        """
        session = self.get_session(session_id)
        session.add_message("user", question)

        intent = self.classify_intent(question, session)
        logger.info(f"Session: {session_id} | Intent: {intent} | Query: {question} | Filters: {filters}")

        answer = ""

        if intent == "GARBAGE":
            answer = "‚ùå C√¢u h·ªèi kh√¥ng h·ª£p l·ªá ho·∫∑c qu√° ng·∫Øn."

        elif intent == "SMALLTALK":
            answer = self.answer_smalltalk(question, session)

        elif intent == "FOLLOWUP":
            answer = self.answer_followup(question, session)

        elif intent == "STATS":
            total = self.vector_db.get_collection_stats().get("count", 0)
            answer = f"üìö Hi·ªán t·∫°i th∆∞ vi·ªán c√≥ **{total} cu·ªën s√°ch** trong h·ªá th·ªëng."

        else:  # SEARCH
            if self.is_library_info_query(question):
                answer = self._generate_library_info_answer(question, session)
            else:
                # Normalize topic queries v√† truy·ªÅn filters t·ª´ FE
                normalized_query = self._normalize_book_query(question)
                answer = self._perform_book_search(normalized_query, session, filters=filters)

        session.add_message("model", answer)
        return answer

    # ==================================================
    # SUB-HANDLERS
    # ==================================================
    def is_library_info_query(self, q: str) -> bool:
        return any(k in q.lower() for k in ["m·ªü c·ª≠a", "quy ƒë·ªãnh", "m∆∞·ª£n s√°ch", "tr·∫£ s√°ch", "ph√≠ ph·∫°t"])

    def _generate_library_info_answer(self, question: str, session: ChatSession) -> str:
        ctx = self._build_library_context()
        prompt = f"""{SYSTEM_PROMPT}\n{USER_PROMPT_TEMPLATE.format(question=question, books="(Kh√¥ng √°p d·ª•ng)", **ctx)}"""
        return self._call_gemini(prompt)

    def _perform_book_search(self, question: str, session: ChatSession, filters: dict = None) -> str:
        q_vec = self.embedder.embed_text(question, is_query=True)

        # TH√äM: Smart cache skip (t·ª´ HEAD)
        # Skip cache n·∫øu c√≥ filters (ƒë·ªÉ ƒë·∫£m b·∫£o k·∫øt qu·∫£ ch√≠nh x√°c)
        if q_vec and not filters:
            cached = self.vector_db.search_query_memory(q_vec, threshold=QUERY_CACHE_THRESHOLD)
            if cached:
                # Skip cache n·∫øu cache l√† s√°ch nh∆∞ng query kh√¥ng li√™n quan s√°ch
                is_book_cache = "üìö" in cached or "Danh s√°ch s√°ch" in cached
                if is_book_cache and not self._is_book_related_query(question):
                    logger.info("‚ö†Ô∏è Query memory SKIP (cached books for non-book query)")
                else:
                    logger.info("‚ö° Query memory HIT")
                    return f"‚ö° {cached}"

        # Search v·ªõi filters n·∫øu ƒë∆∞·ª£c cung c·∫•p
        raw_docs = self.search_engine.search(query=question, filters=filters, top_k=self.top_k * SEARCH_EXPAND_FACTOR)
        if not raw_docs:
            return self._gemini_fallback(question, session)

        best_score = max(d.get("score", 0) for d in raw_docs)
        if best_score < SCORE_THRESHOLD:
            return self._gemini_fallback(question, session)

        docs = raw_docs[:self.top_k]

        session.last_search_results = docs
        session.save()

        book_lines = [
            f"{i}. {d['title']} ‚Äì {d['authors']} ({d['publish_year']})"
            for i, d in enumerate(docs, 1)
        ]
        books_text = "\n".join(book_lines)

        if not self.needs_synthesis(question):
            answer = f"üìö Danh s√°ch s√°ch li√™n quan:\n\n{books_text}"
            if q_vec:
                self.vector_db.add_query_memory(question, q_vec, answer, qtype="rag_list")
            return answer

        ctx = self._build_library_context()
        prompt = f"""{SYSTEM_PROMPT}\n{USER_PROMPT_TEMPLATE.format(question=question, books=books_text, **ctx)}"""

        synthesis = self._call_gemini(prompt)
        answer = f"üìö Danh s√°ch s√°ch li√™n quan:\n\n{books_text}\n\nüìù T·ªïng h·ª£p:\n{synthesis}"

        if q_vec:
            self.vector_db.add_query_memory(question, q_vec, answer, qtype="rag_synthesis")
        return answer

    def _gemini_fallback(self, question: str, session: ChatSession) -> str:
        """TH√äM: D√πng GENERAL_QA_PROMPT_TEMPLATE ƒë·ªÉ tr·∫£ l·ªùi th√¥ng minh h∆°n (t·ª´ HEAD)"""
        prompt = GENERAL_QA_PROMPT_TEMPLATE.format(
            history=session.get_history_text(),
            question=question
        )
        return self._call_gemini(prompt)

    def _call_gemini(self, prompt: str, temperature: float = None, max_tokens: int = None) -> str:
        """Call Gemini via ModelManager (handles rotation & rate limits)"""
        try:
            result = self.model_manager.generate_content(
                prompt=prompt,
                temperature=temperature or TEMPERATURE,
                max_tokens=max_tokens or MAX_OUTPUT_TOKENS
            )
            return result if result else "‚ùå Xin l·ªói, kh√¥ng c√≥ ph·∫£n h·ªìi."
        except Exception as e:
            logger.error(f"Gemini API error: {e}")
            return "‚ùå H·ªá th·ªëng ƒëang b·∫≠n ho·∫∑c g·∫∑p s·ª± c·ªë k·∫øt n·ªëi."

    # ==================================================
    # üÜï SUGGESTED QUESTIONS (TH√äM T·ª™ HEAD)
    # ==================================================
    def get_suggested_questions(self) -> List[str]:
        """Danh s√°ch g·ª£i √Ω m·∫∑c ƒë·ªãnh cho giao di·ªán chat"""
        return [
            "Th∆∞ vi·ªán m·ªü c·ª≠a l√∫c m·∫•y gi·ªù?",
            "L√†m sao ƒë·ªÉ gia h·∫°n s√°ch?",
            "C√≥ s√°ch n√†o v·ªÅ tr√≠ tu·ªá nh√¢n t·∫°o kh√¥ng?",
            "Ph√≠ ph·∫°t tr·∫£ s√°ch tr·ªÖ l√† bao nhi√™u?",
        ]